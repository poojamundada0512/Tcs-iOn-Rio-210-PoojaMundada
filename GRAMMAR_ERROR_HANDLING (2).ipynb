{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SO6vMcfHYi1"
      },
      "outputs": [],
      "source": [
        "#USEFUL LINKS\n",
        "# LINK TO REPLACE WORD IN SENTENCE\n",
        "# https://www.programiz.com/python-programming/methods/string/replace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xtYwh6xLP-rR",
        "outputId": "1629d6c9-33d5-4254-ed27-bf2b0bcd9dd9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.7.0'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#all imports\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split \n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk import pos_tag \n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Conv2D, Flatten , Input , Conv1D , Concatenate , MaxPooling1D , Dropout , Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM\n",
        "import datetime\n",
        "\n",
        "from keras.layers import Concatenate\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.layers import Embedding\n",
        "from sklearn.metrics import  f1_score , roc_auc_score\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import nltk.translate.bleu_score as bleu\n",
        "\n",
        "\n",
        "tf.__version__\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSbFUJW3Rq66"
      },
      "source": [
        "# DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "Q75mMFTpQGn2",
        "outputId": "f111270e-9186-4f4e-971e-7f0cbd7424b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(498362, 2)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>correct</th>\n",
              "      <th>incorrect</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>And he took in my favorite subjects like soccer .</td>\n",
              "      <td>And he took in my favorite subject like soccer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Actually , he was the one who let me know abou...</td>\n",
              "      <td>Actually , who let me know about Lang - 8 was ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>His Kanji ability is much better than mine .</td>\n",
              "      <td>His Kanji 's ability is much better than me .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>We 've known each other for only half a year ,...</td>\n",
              "      <td>We 've known each other for only half a year ,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I heard a sentence last night when I was watch...</td>\n",
              "      <td>I heard a sentence last night when I watched TV .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             correct  \\\n",
              "0  And he took in my favorite subjects like soccer .   \n",
              "1  Actually , he was the one who let me know abou...   \n",
              "2       His Kanji ability is much better than mine .   \n",
              "3  We 've known each other for only half a year ,...   \n",
              "4  I heard a sentence last night when I was watch...   \n",
              "\n",
              "                                           incorrect  \n",
              "0   And he took in my favorite subject like soccer .  \n",
              "1  Actually , who let me know about Lang - 8 was ...  \n",
              "2      His Kanji 's ability is much better than me .  \n",
              "3  We 've known each other for only half a year ,...  \n",
              "4  I heard a sentence last night when I watched TV .  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv('new_data.csv')\n",
        "print(data.shape)\n",
        "data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zt2zH7uMS8Ud",
        "outputId": "eec58b90-386f-403a-f8a6-901dee8827bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ckeck if any missing value is present\n",
        "data.isnull().values.any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-x_WJFjS8Uf",
        "outputId": "43739416-ce27-4244-e8c4-b38e5f68dde4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(498360, 2)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.dropna(inplace=True)\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b93F5NuS8Ug",
        "outputId": "4fe36c64-4417-4330-ee5b-be5e52732306"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(496339, 2)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.drop_duplicates(inplace=True)\n",
        "data.reset_index(inplace=True,drop=True)\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8Xave06S8Ui"
      },
      "outputs": [],
      "source": [
        "def clean(text):\n",
        "    \"\"\"\n",
        "    takes string as input and\n",
        "    removes characters inside (),{},[] and <>\n",
        "    removes characters like -+@#^/|*(){}$~`\n",
        "    we not not removing ,.!-:;\"' as these characters are present in english language \n",
        "    \"\"\"\n",
        "    text = re.sub('<.*>', '', text)\n",
        "    text = re.sub('\\(.*\\)', '', text)\n",
        "    text = re.sub('\\[.*\\]', '', text)\n",
        "    text = re.sub('{.*}', '', text)\n",
        "    text = re.sub(\"[-+@#^/|*(){}$~`<>=_]\",\"\",text)\n",
        "    text = text.replace(\"\\\\\",\"\")\n",
        "    text = re.sub(\"\\[\",\"\",text)\n",
        "    text = re.sub(\"\\]\",\"\",text)\n",
        "    text = re.sub(\"[0-9]\",\"\",text)\n",
        "    return text\n",
        "\n",
        "data[\"correct\"] = data[\"correct\"].apply(clean)\n",
        "data[\"incorrect\"] = data[\"incorrect\"].apply(clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6p_OTtScS8Uj"
      },
      "outputs": [],
      "source": [
        "def percentile(low,high,step,list_temp):\n",
        "    \"\"\"\n",
        "    this function takes low, high, step size as input and prints percentiles accordingly\n",
        "    \"\"\"\n",
        "    for i in np.arange(low,high,step):\n",
        "        print(i,\"percentile is \",np.percentile(list_temp ,i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJQqnDuES8Uk",
        "outputId": "8b6735e1-cd46-4942-f791-e176ef33803c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "496339"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def sen_to_char(sen):\n",
        "    return len([i for i in sen])\n",
        "\n",
        "corr_length = data[\"correct\"].apply(sen_to_char)\n",
        "corr_length = list(corr_length)\n",
        "len(corr_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HBsk9DRS8Ul",
        "outputId": "a74af034-b8b9-43ef-de89-9947dbee8b41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 percentile is  0.0\n",
            "10 percentile is  27.0\n",
            "20 percentile is  35.0\n",
            "30 percentile is  42.0\n",
            "40 percentile is  48.0\n",
            "50 percentile is  55.0\n",
            "60 percentile is  63.0\n",
            "70 percentile is  73.0\n",
            "80 percentile is  86.0\n",
            "90 percentile is  108.0\n",
            "100 percentile is  2622.0\n",
            "***************************************************************\n",
            "90 percentile is  108.0\n",
            "91 percentile is  112.0\n",
            "92 percentile is  116.0\n",
            "93 percentile is  120.0\n",
            "94 percentile is  125.0\n",
            "95 percentile is  131.0\n",
            "96 percentile is  139.0\n",
            "97 percentile is  149.0\n",
            "98 percentile is  163.0\n",
            "99 percentile is  188.0\n",
            "100 percentile is  2622.0\n"
          ]
        }
      ],
      "source": [
        "percentile(0,101,10,corr_length)\n",
        "print(\"***************************************************************\")\n",
        "percentile(90,101,1,corr_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCZOmPpHS8Um",
        "outputId": "405b0ec7-aa96-4af6-8428-8b7ffdd8e56a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(446785, 2)\n"
          ]
        }
      ],
      "source": [
        "# removing those data points which have correct sentence of length more than 108\n",
        "index = []\n",
        "for i in range(len(corr_length)):\n",
        "    if corr_length[i] > 108:\n",
        "        index.append(i)\n",
        "        \n",
        "data.drop(index,inplace=True)\n",
        "data.reset_index(inplace=True,drop=True)\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Cr-Bed9S8Uq",
        "outputId": "1c7d90eb-a630-40a6-9d08-0604da2d94a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "446785"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "incorr_length = data[\"incorrect\"].apply(sen_to_char)\n",
        "incorr_length = list(incorr_length)\n",
        "len(incorr_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "142EBwyOS8Ur",
        "outputId": "6b75025a-406a-4087-bdc7-c8b2f3df7ceb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 percentile is  0.0\n",
            "10 percentile is  25.0\n",
            "20 percentile is  32.0\n",
            "30 percentile is  38.0\n",
            "40 percentile is  44.0\n",
            "50 percentile is  50.0\n",
            "60 percentile is  57.0\n",
            "70 percentile is  64.0\n",
            "80 percentile is  73.0\n",
            "90 percentile is  85.0\n",
            "100 percentile is  611.0\n",
            "***********************************************************\n",
            "90 percentile is  85.0\n",
            "91 percentile is  87.0\n",
            "92 percentile is  89.0\n",
            "93 percentile is  91.0\n",
            "94 percentile is  92.0\n",
            "95 percentile is  95.0\n",
            "96 percentile is  97.0\n",
            "97 percentile is  99.0\n",
            "98 percentile is  103.0\n",
            "99 percentile is  107.0\n",
            "100 percentile is  611.0\n"
          ]
        }
      ],
      "source": [
        "percentile(0,101,10,incorr_length)\n",
        "print(\"***********************************************************\")\n",
        "percentile(90,101,1,incorr_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmfedozwS8Us",
        "outputId": "041ea29c-8615-44fe-82b4-9cc92a76f9c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(443397, 2)\n"
          ]
        }
      ],
      "source": [
        "# removing those data points which have incorrect sentence of length more than 108\n",
        "index = []\n",
        "for i in range(len(incorr_length)):\n",
        "    if incorr_length[i] > 108:\n",
        "        index.append(i)\n",
        "        \n",
        "data.drop(index,inplace=True)\n",
        "data.reset_index(inplace=True,drop=True)\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdjJedu3S8Ut"
      },
      "outputs": [],
      "source": [
        "corr_length = data[\"correct\"].str.split().apply(len)\n",
        "corr_length = list(corr_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_whZ8B8S8Uv",
        "outputId": "82021286-7986-48a9-975f-bf88aad55a0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 percentile is  0.0\n",
            "10 percentile is  6.0\n",
            "20 percentile is  8.0\n",
            "30 percentile is  9.0\n",
            "40 percentile is  10.0\n",
            "50 percentile is  11.0\n",
            "60 percentile is  13.0\n",
            "70 percentile is  14.0\n",
            "80 percentile is  16.0\n",
            "90 percentile is  18.0\n",
            "100 percentile is  47.0\n",
            "***************************************************************\n",
            "90 percentile is  18.0\n",
            "91 percentile is  19.0\n",
            "92 percentile is  19.0\n",
            "93 percentile is  19.0\n",
            "94 percentile is  20.0\n",
            "95 percentile is  20.0\n",
            "96 percentile is  21.0\n",
            "97 percentile is  21.0\n",
            "98 percentile is  22.0\n",
            "99 percentile is  23.0\n",
            "100 percentile is  47.0\n"
          ]
        }
      ],
      "source": [
        "percentile(0,101,10,corr_length)\n",
        "print(\"***************************************************************\")\n",
        "percentile(90,101,1,corr_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9u16HYzS8Uw",
        "outputId": "32cffb3e-0c0d-469d-d0f6-2a4667729712"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(441931, 2)\n"
          ]
        }
      ],
      "source": [
        "# removing those data points which have correct sentence of length more than 24\n",
        "index = []\n",
        "for i in range(len(corr_length)):\n",
        "    if corr_length[i] > 24:\n",
        "        index.append(i)\n",
        "        \n",
        "data.drop(index,inplace=True)\n",
        "data.reset_index(inplace=True,drop=True)\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QJ7JFu4S8Ux"
      },
      "outputs": [],
      "source": [
        "incorr_length = data[\"incorrect\"].str.split().apply(len)\n",
        "incorr_length = list(incorr_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEFMVZnjS8Uy",
        "outputId": "3e77b8b7-14e8-44f9-94e9-65e3be9072ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 percentile is  0.0\n",
            "10 percentile is  6.0\n",
            "20 percentile is  7.0\n",
            "30 percentile is  8.0\n",
            "40 percentile is  10.0\n",
            "50 percentile is  11.0\n",
            "60 percentile is  12.0\n",
            "70 percentile is  14.0\n",
            "80 percentile is  15.0\n",
            "90 percentile is  18.0\n",
            "100 percentile is  31.0\n",
            "***************************************************************\n",
            "90 percentile is  18.0\n",
            "91 percentile is  18.0\n",
            "92 percentile is  18.0\n",
            "93 percentile is  19.0\n",
            "94 percentile is  19.0\n",
            "95 percentile is  20.0\n",
            "96 percentile is  20.0\n",
            "97 percentile is  21.0\n",
            "98 percentile is  21.0\n",
            "99 percentile is  22.0\n",
            "100 percentile is  31.0\n"
          ]
        }
      ],
      "source": [
        "percentile(0,101,10,incorr_length)\n",
        "print(\"***************************************************************\")\n",
        "percentile(90,101,1,incorr_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNi-kVcvS8Uy",
        "outputId": "3a4db913-df25-4517-ce4f-6347f6ab6749"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(441842, 2)\n"
          ]
        }
      ],
      "source": [
        "# removing those data points which have incorrect sentence of length more than 25\n",
        "index = []\n",
        "for i in range(len(incorr_length)):\n",
        "    if incorr_length[i] > 25:\n",
        "        index.append(i)\n",
        "        \n",
        "data.drop(index,inplace=True)\n",
        "data.reset_index(inplace=True,drop=True)\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rA91MgC_S8Uz"
      },
      "outputs": [],
      "source": [
        "data[\"correct\"] = data[\"correct\"].astype(str)\n",
        "data[\"incorrect\"] = data[\"incorrect\"].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YP5rbIrmQpVd",
        "outputId": "3ca18661-f1ed-4c9a-eeb3-050853ab3720"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(pandas.core.series.Series, 0)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(data.correct) , data.isnull().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBzZDw87QmL1",
        "outputId": "0e50bd4e-34dd-48e1-a87f-55e3c947b47d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████| 441842/441842 [00:00<00:00, 2889520.02it/s]\n",
            "100%|█████████████████████████████████████████████████████████████████████| 441842/441842 [00:00<00:00, 3365213.43it/s]\n",
            "100%|█████████████████████████████████████████████████████████████████████| 441842/441842 [00:00<00:00, 3355215.39it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(108, 0, 53.660100216819586)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "length_max = max([len(i) for i in tqdm(data['correct'])])\n",
        "length_min = min([len(i) for i in tqdm(data['correct'])])\n",
        "avg = [len(i) for i in tqdm(data['correct'])]\n",
        "length_avg = np.array([avg]).mean()\n",
        "\n",
        "length_max , length_min , length_avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "255HrENRQvfZ"
      },
      "outputs": [],
      "source": [
        "CORRECT_SENTENCE_LEN = data['correct'].str.split().apply(len) \n",
        "ERRONEOUS_SENTENCE_LEN = data['incorrect'].str.split().apply(len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_b7WjeERiSJ",
        "outputId": "18f81daf-5d10-40bb-e460-ff9a8c49a37f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 0.0\n",
            "10 6.0\n",
            "20 7.0\n",
            "30 8.0\n",
            "40 10.0\n",
            "50 11.0\n",
            "60 12.0\n",
            "70 14.0\n",
            "80 15.0\n",
            "90 18.0\n",
            "100 25.0\n",
            "90 18.0\n",
            "91 18.0\n",
            "92 18.0\n",
            "93 19.0\n",
            "94 19.0\n",
            "95 20.0\n",
            "96 20.0\n",
            "97 21.0\n",
            "98 21.0\n",
            "99 22.0\n",
            "100 25.0\n",
            "99.1 22.0\n",
            "99.2 23.0\n",
            "99.3 23.0\n",
            "99.4 23.0\n",
            "99.5 23.0\n",
            "99.6 23.0\n",
            "99.7 24.0\n",
            "99.8 24.0\n",
            "99.9 24.0\n",
            "100 25.0\n"
          ]
        }
      ],
      "source": [
        "for i in range(0,101,10):\n",
        "    print(i,np.percentile(ERRONEOUS_SENTENCE_LEN, i))\n",
        "for i in range(90,101):\n",
        "    print(i,np.percentile(ERRONEOUS_SENTENCE_LEN, i))\n",
        "for i in [99.1,99.2,99.3,99.4,99.5,99.6,99.7,99.8,99.9,100]:\n",
        "    print(i,np.percentile(ERRONEOUS_SENTENCE_LEN, i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E6rKnexSM6o"
      },
      "source": [
        "SINCE 99.9% OF DATA HAS LENGTH LESS THAN 10 , SO SELECTING SENTENCE WITH WORD <10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "ZFwGWKh6SMiy",
        "outputId": "34561849-c4d6-495a-c2e6-755b5cb9be39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(409715, 3)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>incorrect</th>\n",
              "      <th>english_inp</th>\n",
              "      <th>english_out</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>And he took in my favorite subject like soccer .</td>\n",
              "      <td>&lt;start&gt; And he took in my favorite subjects li...</td>\n",
              "      <td>And he took in my favorite subjects like socce...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Actually , who let me know about Lang   was him .</td>\n",
              "      <td>&lt;start&gt; Actually , he was the one who let me k...</td>\n",
              "      <td>Actually , he was the one who let me know abou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>His Kanji 's ability is much better than me .</td>\n",
              "      <td>&lt;start&gt; His Kanji ability is much better than ...</td>\n",
              "      <td>His Kanji ability is much better than mine . &lt;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I heard a sentence last night when I watched TV .</td>\n",
              "      <td>&lt;start&gt; I heard a sentence last night when I w...</td>\n",
              "      <td>I heard a sentence last night when I was watch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>When you go downhill , you have to stick out y...</td>\n",
              "      <td>&lt;start&gt; When you go downhill , you have to sti...</td>\n",
              "      <td>When you go downhill , you have to stick out y...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           incorrect  \\\n",
              "0   And he took in my favorite subject like soccer .   \n",
              "1  Actually , who let me know about Lang   was him .   \n",
              "2      His Kanji 's ability is much better than me .   \n",
              "4  I heard a sentence last night when I watched TV .   \n",
              "5  When you go downhill , you have to stick out y...   \n",
              "\n",
              "                                         english_inp  \\\n",
              "0  <start> And he took in my favorite subjects li...   \n",
              "1  <start> Actually , he was the one who let me k...   \n",
              "2  <start> His Kanji ability is much better than ...   \n",
              "4  <start> I heard a sentence last night when I w...   \n",
              "5  <start> When you go downhill , you have to sti...   \n",
              "\n",
              "                                         english_out  \n",
              "0  And he took in my favorite subjects like socce...  \n",
              "1  Actually , he was the one who let me know abou...  \n",
              "2  His Kanji ability is much better than mine . <...  \n",
              "4  I heard a sentence last night when I was watch...  \n",
              "5  When you go downhill , you have to stick out y...  "
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['CORRECT_SENTENCE_LEN'] = data['correct'].str.split().apply(len)\n",
        "data = data[data['CORRECT_SENTENCE_LEN'] < 20]\n",
        "\n",
        "data['ERRONEOUS_SENTENCE_LEN'] = data['incorrect'].str.split().apply(len)\n",
        "data = data[data['ERRONEOUS_SENTENCE_LEN'] < 20]\n",
        "\n",
        "#ADDING start and end IN THE SENTENCES\n",
        "data['english_inp'] = '<start> ' + data['correct'].astype(str)\n",
        "data['english_out'] = data['correct'].astype(str) + ' <end>'\n",
        "\n",
        "data = data.drop(['correct','CORRECT_SENTENCE_LEN','ERRONEOUS_SENTENCE_LEN'], axis=1)\n",
        "print(data.shape)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM8ciYtJWK6f"
      },
      "source": [
        "### Getting train and test "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfrDw7tzT5Qx"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, validation = train_test_split(data, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySroEsmXWOFW",
        "outputId": "c6f8e3e7-8cbe-45be-a3b5-28a5a81f8e16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(327772, 3) (81943, 3)\n"
          ]
        }
      ],
      "source": [
        "print(train.shape, validation.shape)\n",
        "#ADDING TO <end> TO ONE OF THE SENTENCES SO THAT TOKENIZER LEARNS THE WORD <end>\n",
        "train.iloc[0]['english_inp']= str(train.iloc[0]['english_inp'])+' <end>'\n",
        "train.iloc[0]['english_out']= str(train.iloc[0]['english_out'])+' <end>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b05db-RnH_NV",
        "outputId": "1d70e73a-efc2-47e5-d813-f4fef037120b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████| 327772/327772 [00:00<00:00, 2177748.19it/s]\n",
            "100%|█████████████████████████████████████████████████████████████████████| 327772/327772 [00:00<00:00, 2533625.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "108 116\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "max_encoder_length = max([len(i) for i in tqdm(train['incorrect'])])\n",
        "max_decoder_length = max([len(i) for i in tqdm(train['english_inp'])])\n",
        "print(max_encoder_length , max_decoder_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "Su69ZPzTsxmn",
        "outputId": "89a8ff4e-3552-4ed8-b554-3174901bb441"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>incorrect</th>\n",
              "      <th>english_inp</th>\n",
              "      <th>english_out</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>266458</th>\n",
              "      <td>Last week , my university did graduation .</td>\n",
              "      <td>&lt;start&gt; Last week , my university had their gr...</td>\n",
              "      <td>Last week , my university had their graduation...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404826</th>\n",
              "      <td>When I was little , she always made clothes fo...</td>\n",
              "      <td>&lt;start&gt; When I was little , she always made cl...</td>\n",
              "      <td>When I was little , she always made clothes fo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                incorrect  \\\n",
              "266458         Last week , my university did graduation .   \n",
              "404826  When I was little , she always made clothes fo...   \n",
              "\n",
              "                                              english_inp  \\\n",
              "266458  <start> Last week , my university had their gr...   \n",
              "404826  <start> When I was little , she always made cl...   \n",
              "\n",
              "                                              english_out  \n",
              "266458  Last week , my university had their graduation...  \n",
              "404826  When I was little , she always made clothes fo...  "
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.sample(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "CGqQDR8FWV3D",
        "outputId": "12c6849b-6039-4058-b433-ea4083384e47"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>incorrect</th>\n",
              "      <th>english_inp</th>\n",
              "      <th>english_out</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>177050</th>\n",
              "      <td>Now even I 'm thinking about studying abroad i...</td>\n",
              "      <td>&lt;start&gt; Now I 'm even thinking about studying ...</td>\n",
              "      <td>Now I 'm even thinking about studying abroad i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>238422</th>\n",
              "      <td>So I do n't know this system .</td>\n",
              "      <td>&lt;start&gt; So I do n't know how this system works .</td>\n",
              "      <td>So I do n't know how this system works . &lt;end&gt;</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                incorrect  \\\n",
              "177050  Now even I 'm thinking about studying abroad i...   \n",
              "238422                     So I do n't know this system .   \n",
              "\n",
              "                                              english_inp  \\\n",
              "177050  <start> Now I 'm even thinking about studying ...   \n",
              "238422   <start> So I do n't know how this system works .   \n",
              "\n",
              "                                              english_out  \n",
              "177050  Now I 'm even thinking about studying abroad i...  \n",
              "238422     So I do n't know how this system works . <end>  "
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "validation.sample(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnjtTfVOJLnA"
      },
      "source": [
        "**TOKENIING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPa0ldDWkav8"
      },
      "outputs": [],
      "source": [
        "tknizer_ERRONEOUS_SENTENCE = Tokenizer()\n",
        "tknizer_ERRONEOUS_SENTENCE.fit_on_texts(train['incorrect'].values)\n",
        "tknizer_CORRECT_SENTENCE = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
        "tknizer_CORRECT_SENTENCE.fit_on_texts(train['english_inp'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBb0yEYlCynz",
        "outputId": "dbc917e3-648b-4bb3-8447-5059152efa96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "45162\n",
            "58998\n"
          ]
        }
      ],
      "source": [
        "vocab_size_CORRECT_SENTENCE=len(tknizer_CORRECT_SENTENCE.word_index.keys())\n",
        "print(vocab_size_CORRECT_SENTENCE)\n",
        "vocab_size_ERRONEOUS_SENTENCE=len(tknizer_ERRONEOUS_SENTENCE.word_index.keys())\n",
        "print(vocab_size_ERRONEOUS_SENTENCE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nm1uoAC-Nwuk",
        "outputId": "9b4247b5-8c76-4c5f-e2f6-9035d192ef5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 23129)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tknizer_CORRECT_SENTENCE.word_index['<start>'], tknizer_CORRECT_SENTENCE.word_index['<end>']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jnVtXGEJglN"
      },
      "source": [
        "TOKENIZER WITH ENGLISH WORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKInpM9fI7TA",
        "outputId": "578ae5f4-a2fc-4f01-a784-624a8473162b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(45163, 100)"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings_index = dict()\n",
        "f = open('glove.6B.100d.txt',encoding=\"utf8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size_CORRECT_SENTENCE+1, 100))\n",
        "for word, i in tknizer_CORRECT_SENTENCE.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "embedding_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48GmIsiBuV-N"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "impressive-advancement"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, embedding_dim, input_length, enc_units):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.input_length = input_length\n",
        "        self.enc_units= enc_units\n",
        "        self.lstm_output = 0\n",
        "        self.lstm_state_h=0\n",
        "        self.lstm_state_c=0\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, input_length=self.input_length,\n",
        "                           mask_zero=True, name=\"embedding_layer_encoder\", input_shape=(self.vocab_size,))\n",
        "        self.lstm = LSTM(self.enc_units, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
        "        \n",
        "    def call(self, input_sentances, training=True):\n",
        "        input_embedd                        = self.embedding(input_sentances)\n",
        "        self.lstm_output, self.lstm_state_h,self.lstm_state_c = self.lstm(input_embedd)\n",
        "        return self.lstm_output, self.lstm_state_h,self.lstm_state_c\n",
        "    def get_states(self):\n",
        "        return self.lstm_state_h,self.lstm_state_c\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "effective-world"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, embedding_dim, input_length, dec_units):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.dec_units = dec_units\n",
        "        self.input_length = input_length\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        # we are using embedding_matrix weights and not training the embedding layer\n",
        "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim, input_length=self.input_length,\n",
        "                           mask_zero=True, name=\"embedding_layer_decoder\", weights=[embedding_matrix],input_shape=(self.vocab_size,))\n",
        "        self.lstm = LSTM(self.dec_units, return_sequences=True, return_state=True, name=\"Encoder_LSTM\")\n",
        "        \n",
        "    def call(self, target_sentances, state_h, state_c):\n",
        "        target_embedd           = self.embedding(target_sentances)\n",
        "        lstm_output, _,_        = self.lstm(target_embedd, initial_state=[state_h, state_c])\n",
        "        return lstm_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cathedral-graham"
      },
      "source": [
        "## Data pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "macro-senator"
      },
      "outputs": [],
      "source": [
        "class Dataset:\n",
        "    def __init__(self, data, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, max_len):\n",
        "        self.encoder_inps = data['incorrect'].values\n",
        "        self.decoder_inps = data['english_inp'].values\n",
        "        self.decoder_outs = data['english_out'].values\n",
        "        self.tknizer_CORRECT_SENTENCE = tknizer_CORRECT_SENTENCE\n",
        "        self.tknizer_ERRONEOUS_SENTENCE = tknizer_ERRONEOUS_SENTENCE\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        self.encoder_seq = self.tknizer_ERRONEOUS_SENTENCE.texts_to_sequences([self.encoder_inps[i]]) # need to pass list of values\n",
        "        self.decoder_inp_seq = self.tknizer_CORRECT_SENTENCE.texts_to_sequences([self.decoder_inps[i]])\n",
        "        self.decoder_out_seq = self.tknizer_CORRECT_SENTENCE.texts_to_sequences([self.decoder_outs[i]])\n",
        "\n",
        "        self.encoder_seq = pad_sequences(self.encoder_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
        "        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
        "        self.decoder_out_seq = pad_sequences(self.decoder_out_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
        "        return self.encoder_seq, self.decoder_inp_seq, self.decoder_out_seq\n",
        "\n",
        "    def __len__(self): # your model.fit_gen requires this function\n",
        "        return len(self.encoder_inps)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "featured-offense"
      },
      "outputs": [],
      "source": [
        "class Dataloder(tf.keras.utils.Sequence):\n",
        "    \n",
        "    def __init__(self, dataset, batch_size=1):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.indexes = np.arange(len(self.dataset.encoder_inps))\n",
        "\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        start = i * self.batch_size\n",
        "        stop = (i + 1) * self.batch_size\n",
        "        data = []\n",
        "        for j in range(start, stop):\n",
        "            data.append(self.dataset[j])\n",
        "\n",
        "        batch = [np.squeeze(np.stack(samples, axis=1), axis=0) for samples in zip(*data)]\n",
        "        \n",
        "        # we are creating data like ([italian, english_inp], english_out) these are already converted into seq\n",
        "        \n",
        "        return [batch[0],batch[1]],batch[2]\n",
        "\n",
        "    def __len__(self):  # your model.fit_gen requires this function\n",
        "        return len(self.indexes) // self.batch_size\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = np.random.permutation(self.indexes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "medium-letter"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "horizontal-links"
      },
      "outputs": [],
      "source": [
        "class vanilla_model(Model):\n",
        "    def __init__(self, encoder_inputs_length,decoder_inputs_length, output_vocab_size):\n",
        "        super().__init__() \n",
        "        self.encoder = Encoder(vocab_size=vocab_size_ERRONEOUS_SENTENCE + 1, embedding_dim=100, input_length=encoder_inputs_length, enc_units=256)\n",
        "        self.decoder = Decoder(vocab_size=vocab_size_CORRECT_SENTENCE + 1, embedding_dim=100, input_length=decoder_inputs_length, dec_units=256)\n",
        "        self.dense   = Dense(output_vocab_size, activation='softmax')\n",
        "        \n",
        "        \n",
        "    def call(self, data):\n",
        "        input,output = data[0], data[1]\n",
        "        encoder_output, encoder_h, encoder_c = self.encoder(input)\n",
        "        decoder_output                       = self.decoder(output, encoder_h, encoder_c)\n",
        "        output                               = self.dense(decoder_output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neither-tennis",
        "outputId": "9fc3898e-752e-4d4e-f077-101cea1d380d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(512, 20) (512, 20) (512, 20)\n"
          ]
        }
      ],
      "source": [
        "train_dataset = Dataset(train, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, 16)\n",
        "test_dataset  = Dataset(validation, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, 16)\n",
        "\n",
        "train_dataset = Dataset(train, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, 20)\n",
        "test_dataset  = Dataset(validation, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, 20)\n",
        "\n",
        "train_dataloader = Dataloder(train_dataset, batch_size=512)\n",
        "test_dataloader = Dataloder(test_dataset, batch_size=512)\n",
        "\n",
        "print(train_dataloader[0][0][0].shape, train_dataloader[0][0][1].shape, train_dataloader[0][1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "postal-portrait"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "earlier-africa"
      },
      "outputs": [],
      "source": [
        "vanilla = vanilla_model(encoder_inputs_length=16,decoder_inputs_length=16,output_vocab_size=vocab_size_CORRECT_SENTENCE)\n",
        "optimizer = tf.keras.optimizers.Adam(clipnorm=1.0)\n",
        "vanilla.compile(optimizer= optimizer, loss= loss_function, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fewer-career"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "# logfile = \"/content/Project_2/logs/vanilla/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# #logfile = \"C:\\\\Users\\\\rpris\\\\Google Drive\\\\Project_2\\\\logs\\\\vanilla\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# tfboard = tf.keras.callbacks.TensorBoard(log_dir=logfile, histogram_freq=1, write_graph=True)\n",
        "\n",
        "# chkfile = \"/content/wts/vanilla/weights-{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
        "# #chkfile2 = \"C:\\\\Users\\\\rpris\\\\Google Drive\\\\Project_2\\\\wts\\\\vanilla\\\\weights-{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
        "# chkpt = tf.keras.callbacks.ModelCheckpoint(chkfile, monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=0, mode='min')\n",
        "\n",
        "# stp = tf.keras.callbacks.EarlyStopping(patience=7, monitor='val_loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUqzPMbcjsc4"
      },
      "outputs": [],
      "source": [
        "# vanilla.load_weights(\"/content/drive/MyDrive/Project_2/wts/vanilla/weights-65-0.5898.hdf5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LNgdV8GlsDy"
      },
      "outputs": [],
      "source": [
        "# vanilla.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmSmx9Q0S8VE",
        "outputId": "e67bcfd8-823a-4d08-a88e-8d05dade9d61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "320 80\n"
          ]
        }
      ],
      "source": [
        "train_steps=train.shape[0]//1024\n",
        "valid_steps=validation.shape[0]//1024\n",
        "print(train_steps,valid_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjcMmTHC0S9o",
        "outputId": "1d91a3f9-1524-49ae-d88d-24e27632465a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\kiran\\AppData\\Local\\Temp\\ipykernel_25572\\871002888.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  vanilla.fit_generator(train_dataloader, steps_per_epoch=train_steps, epochs=10 , validation_data=train_dataloader, validation_steps=valid_steps )#, callbacks=[stp, chkpt, tfboard]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "320/320 [==============================] - 2499s 8s/step - loss: 3.2887 - val_loss: 3.0354\n",
            "Epoch 2/10\n",
            "320/320 [==============================] - 3216s 10s/step - loss: 2.8410 - val_loss: 2.6673\n",
            "Epoch 3/10\n",
            "320/320 [==============================] - 2655s 8s/step - loss: 2.5385 - val_loss: 2.4194\n",
            "Epoch 4/10\n",
            "320/320 [==============================] - 2798s 9s/step - loss: 2.3336 - val_loss: 2.2396\n",
            "Epoch 5/10\n",
            "320/320 [==============================] - 2801s 9s/step - loss: 2.1617 - val_loss: 2.0676\n",
            "Epoch 6/10\n",
            "320/320 [==============================] - 2749s 9s/step - loss: 1.9889 - val_loss: 1.9018\n",
            "Epoch 7/10\n",
            "320/320 [==============================] - 2823s 9s/step - loss: 1.8417 - val_loss: 1.7641\n",
            "Epoch 8/10\n",
            "320/320 [==============================] - 2665s 8s/step - loss: 1.7206 - val_loss: 1.6478\n",
            "Epoch 9/10\n",
            "320/320 [==============================] - 2676s 8s/step - loss: 1.6114 - val_loss: 1.5484\n",
            "Epoch 10/10\n",
            "320/320 [==============================] - 2672s 8s/step - loss: 1.5129 - val_loss: 1.4512\n",
            "Model: \"vanilla_model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder_2 (Encoder)         multiple                  6265468   \n",
            "                                                                 \n",
            " decoder_2 (Decoder)         multiple                  4881868   \n",
            "                                                                 \n",
            " dense_2 (Dense)             multiple                  11606634  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22,753,970\n",
            "Trainable params: 22,753,970\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "vanilla  = vanilla_model(encoder_inputs_length=20,decoder_inputs_length=20,output_vocab_size=vocab_size_CORRECT_SENTENCE)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "vanilla.compile(optimizer=optimizer,loss='sparse_categorical_crossentropy')\n",
        "train_steps=train.shape[0]//1024\n",
        "valid_steps=validation.shape[0]//1024\n",
        "#TRANING THE MODEL FOR 50 EPOCHS CAUSE , MORE TRAINING GIVES MORE RESULTS\n",
        "vanilla.fit_generator(train_dataloader, steps_per_epoch=train_steps, epochs=10 , validation_data=train_dataloader, validation_steps=valid_steps )#, callbacks=[stp, chkpt, tfboard]\n",
        "# model_1.fit_generator(train_dataloader,  epochs=4, validation_data=train_dataloader)\n",
        "vanilla.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W36gv_QP0O1U"
      },
      "outputs": [],
      "source": [
        "os.mkdir('saved_model_newww')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mDA7LIXvBwh"
      },
      "outputs": [],
      "source": [
        "vanilla.save_weights('saved_model_newww/vanilla.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6UQOxDPBDeu"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30jMi19gBXv4",
        "outputId": "70a53795-3f9d-45f2-ef66-37020b0f51b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "113/113 [==============================] - 92s 738ms/step - loss: 4.9768 - val_loss: 4.1294\n",
            "Model: \"vanilla_model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_4 (Encoder)          multiple                  10858468  \n",
            "_________________________________________________________________\n",
            "decoder_4 (Decoder)          multiple                  3798768   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              multiple                  8823067   \n",
            "=================================================================\n",
            "Total params: 23,480,303\n",
            "Trainable params: 23,480,303\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "vanilla  = vanilla_model(encoder_inputs_length=20,decoder_inputs_length=20,output_vocab_size=vocab_size_CORRECT_SENTENCE)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "vanilla.compile(optimizer=optimizer,loss='sparse_categorical_crossentropy')\n",
        "train_steps=train.shape[0]//1024\n",
        "valid_steps=validation.shape[0]//1024\n",
        "#TRANING THE MODEL FOR 50 EPOCHS CAUSE , MORE TRAINING GIVES MORE RESULTS\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\"best_Model.h5\",save_best_only=True)\n",
        "vanilla.fit_generator(train_dataloader, steps_per_epoch=train_steps, epochs=1 , validation_data=train_dataloader, validation_steps=valid_steps\\\n",
        "                      , callbacks=[model_checkpoint_callback])#, callbacks=[stp, chkpt, tfboard]\n",
        "# model_1.fit_generator(train_dataloader,  epochs=4, validation_data=train_dataloader)\n",
        "vanilla.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFcQC8r3GD2T"
      },
      "outputs": [],
      "source": [
        "vanilla.load_weights(\"//content/drive/MyDrive/saved_model_newww/vanilla.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3O-hpedF3iN",
        "outputId": "e6cd3a37-72f5-4630-daea-70dce957220a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"vanilla_model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " encoder_2 (Encoder)         multiple                  6265468   \n",
            "                                                                 \n",
            " decoder_2 (Decoder)         multiple                  4881868   \n",
            "                                                                 \n",
            " dense_2 (Dense)             multiple                  11606634  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22,753,970\n",
            "Trainable params: 22,753,970\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "vanilla.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SFjlz-AG1t4"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3otXDvr4-_F",
        "outputId": "6a0bf7fc-af07-40b7-8982-453359c09b33"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Actually , who let me know about Lang   was him .',\n",
              " '<start> Actually , he was the one who let me know about Lang   . .')"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train.incorrect[1] , train.english_inp[1] , "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfuQQ1Kk41Ia"
      },
      "outputs": [],
      "source": [
        "enc_inp = train.incorrect[1]\n",
        "dec_inp =  train.english_inp[1] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HP239Hia4uAE",
        "outputId": "fe41a160-3f05-4154-886d-5929101ef0ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'please please please wants trying first wants wants you see about '"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translation=\"\"\n",
        "\n",
        "e_input=[]\n",
        "for i in enc_inp.split():\n",
        "    if tknizer_ERRONEOUS_SENTENCE.word_index.get(i) == None:\n",
        "        e_input.append(0)\n",
        "    else:\n",
        "        e_input.append(tknizer_ERRONEOUS_SENTENCE.word_index.get(i))\n",
        "\n",
        "#e_input = pad_sequences(e_input, maxlen=16, padding='post')\n",
        "\n",
        "\n",
        "e_output, e_hidden, e_cell = vanilla.layers[0](np.array([e_input], dtype='int32'))\n",
        "\n",
        "#there is no onestep decoder in this thing, so I have to use the decoder input to predict output\n",
        "\n",
        "#decoder input\n",
        "d_input=[]\n",
        "for i in dec_inp.split():\n",
        "    if tknizer_CORRECT_SENTENCE.word_index.get(i) == None:\n",
        "        d_input.append(0)\n",
        "    else:\n",
        "        d_input.append(tknizer_CORRECT_SENTENCE.word_index.get(i))\n",
        "\n",
        "#d_input = pad_sequences(d_input, maxlen=16, padding='post')\n",
        "\n",
        "prediction = vanilla.layers[2](vanilla.layers[1](np.array([d_input], dtype='int32'),e_hidden,e_cell))\n",
        "\n",
        "for word in prediction[0]:\n",
        "    word = tknizer_CORRECT_SENTENCE.index_word[tf.argmax(word).numpy()]\n",
        "    if word == \"<end>\":\n",
        "        break\n",
        "    translation += word + \" \"\n",
        "translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8o6HWhTFkw1"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "nBF1J4nK4f_4",
        "outputId": "afb02441-dd33-4bc2-e84a-82433c0bbe4d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>incorrect</th>\n",
              "      <th>english_inp</th>\n",
              "      <th>english_out</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>436016</th>\n",
              "      <td>Launch was a barbecue .</td>\n",
              "      <td>&lt;start&gt; Lunch was a barbecue .</td>\n",
              "      <td>Lunch was a barbecue . &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158251</th>\n",
              "      <td>I have technical blog , blog at tumblr where I...</td>\n",
              "      <td>&lt;start&gt; I have a technical blog : a blog at tu...</td>\n",
              "      <td>I have a technical blog : a blog at tumblr whe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54934</th>\n",
              "      <td>But the people lined the back of me lend me mo...</td>\n",
              "      <td>&lt;start&gt; But the person in line behind me offer...</td>\n",
              "      <td>But the person in line behind me offered to le...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13291</th>\n",
              "      <td>Also in Japan , there had been a cat which sai...</td>\n",
              "      <td>&lt;start&gt; Also in Japan , there had been a cat w...</td>\n",
              "      <td>Also in Japan , there had been a cat which sai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40676</th>\n",
              "      <td>According to Lind , why did NOT Chunky Monkey ...</td>\n",
              "      <td>&lt;start&gt; According to Lind , why did Chunky Mon...</td>\n",
              "      <td>According to Lind , why did Chunky Monkey NOT ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279767</th>\n",
              "      <td>After that , I 'll study English and watching ...</td>\n",
              "      <td>&lt;start&gt; After that , I 'll study English and w...</td>\n",
              "      <td>After that , I 'll study English and watch mov...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>441661</th>\n",
              "      <td>I do n't know what to do to go through the mis...</td>\n",
              "      <td>&lt;start&gt; I do n't know what to do to survive th...</td>\n",
              "      <td>I do n't know what to do to survive this miser...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>236406</th>\n",
              "      <td>Anyway we really wish we could help them .</td>\n",
              "      <td>&lt;start&gt; Anyway we really wished that we could ...</td>\n",
              "      <td>Anyway we really wished that we could help the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>388531</th>\n",
              "      <td>The look of it is very wild .</td>\n",
              "      <td>&lt;start&gt; It looked very wild .</td>\n",
              "      <td>It looked very wild . &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>232393</th>\n",
              "      <td>Step by step unusual things and coincedenses c...</td>\n",
              "      <td>&lt;start&gt; Step by step unusual things and coinci...</td>\n",
              "      <td>Step by step unusual things and coincidences c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                incorrect  \\\n",
              "436016                            Launch was a barbecue .   \n",
              "158251  I have technical blog , blog at tumblr where I...   \n",
              "54934   But the people lined the back of me lend me mo...   \n",
              "13291   Also in Japan , there had been a cat which sai...   \n",
              "40676   According to Lind , why did NOT Chunky Monkey ...   \n",
              "...                                                   ...   \n",
              "279767  After that , I 'll study English and watching ...   \n",
              "441661  I do n't know what to do to go through the mis...   \n",
              "236406         Anyway we really wish we could help them .   \n",
              "388531                      The look of it is very wild .   \n",
              "232393  Step by step unusual things and coincedenses c...   \n",
              "\n",
              "                                              english_inp  \\\n",
              "436016                     <start> Lunch was a barbecue .   \n",
              "158251  <start> I have a technical blog : a blog at tu...   \n",
              "54934   <start> But the person in line behind me offer...   \n",
              "13291   <start> Also in Japan , there had been a cat w...   \n",
              "40676   <start> According to Lind , why did Chunky Mon...   \n",
              "...                                                   ...   \n",
              "279767  <start> After that , I 'll study English and w...   \n",
              "441661  <start> I do n't know what to do to survive th...   \n",
              "236406  <start> Anyway we really wished that we could ...   \n",
              "388531                      <start> It looked very wild .   \n",
              "232393  <start> Step by step unusual things and coinci...   \n",
              "\n",
              "                                              english_out  \n",
              "436016                       Lunch was a barbecue . <end>  \n",
              "158251  I have a technical blog : a blog at tumblr whe...  \n",
              "54934   But the person in line behind me offered to le...  \n",
              "13291   Also in Japan , there had been a cat which sai...  \n",
              "40676   According to Lind , why did Chunky Monkey NOT ...  \n",
              "...                                                   ...  \n",
              "279767  After that , I 'll study English and watch mov...  \n",
              "441661  I do n't know what to do to survive this miser...  \n",
              "236406  Anyway we really wished that we could help the...  \n",
              "388531                        It looked very wild . <end>  \n",
              "232393  Step by step unusual things and coincidences c...  \n",
              "\n",
              "[1000 rows x 3 columns]"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_train = train.sample(1000)\n",
        "sample_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "T-NAasQ3Q3Jh",
        "outputId": "2b4daa70-2e27-4e83-edc0-f9772737402d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'you you should to you you have to your your your face or you '"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def inference(enc_inp,dec_inp):\n",
        "            \n",
        "    translation=\"\"\n",
        "\n",
        "    e_input=[]\n",
        "    for i in enc_inp.split():\n",
        "        if tknizer_ERRONEOUS_SENTENCE.word_index.get(i) == None:\n",
        "            e_input.append(0)\n",
        "        else:\n",
        "            e_input.append(tknizer_ERRONEOUS_SENTENCE.word_index.get(i))\n",
        "\n",
        "    #e_input = pad_sequences(e_input, maxlen=16, padding='post')\n",
        "\n",
        "\n",
        "    e_output, e_hidden, e_cell = vanilla.layers[0](np.array([e_input], dtype='int32'))\n",
        "\n",
        "    #there is no onestep decoder in this thing, so I have to use the decoder input to predict output\n",
        "\n",
        "    #decoder input\n",
        "    d_input=[]\n",
        "    for i in dec_inp.split():\n",
        "        if tknizer_CORRECT_SENTENCE.word_index.get(i) == None:\n",
        "            d_input.append(0)\n",
        "        else:\n",
        "            d_input.append(tknizer_CORRECT_SENTENCE.word_index.get(i))\n",
        "\n",
        "    #d_input = pad_sequences(d_input, maxlen=16, padding='post')\n",
        "\n",
        "    prediction = vanilla.layers[2](vanilla.layers[1](np.array([d_input], dtype='int32'),e_hidden,e_cell))\n",
        "\n",
        "    for word in prediction[0]:\n",
        "        word = tknizer_CORRECT_SENTENCE.index_word[tf.argmax(word).numpy()]\n",
        "        if word == \"<end>\":\n",
        "            break\n",
        "        translation += word + \" \"\n",
        "    return translation\n",
        "a = train.incorrect[5]\n",
        "b =  train.english_inp[5]\n",
        "pred = inference(a,b)\n",
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7noJ9eFPS8DY",
        "outputId": "5c22f69a-fcd6-4fba-ecfe-99e40493209c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('The making souvenir is a hard and interesting work .',\n",
              " '<start> Making souvenirs is a hard but interesting work .',\n",
              " 'a a is a good job a and ')"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = train.incorrect[10]\n",
        "b =  train.english_inp[10]\n",
        "pred = inference(a,b)\n",
        "a  , b , pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlrh_8nNTDEB",
        "outputId": "cdd5e646-1654-4041-985b-65fa8c802ea8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(\"That 's two balls : one with a cat inside , other with dog .\",\n",
              " '<start> There are two balls : one with a cat inside , the other with dog .',\n",
              " 'two two two two a a piece a lot with of of and and her ')"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = train.incorrect[12]\n",
        "b =  train.english_inp[12]\n",
        "pred = inference(a,b)\n",
        "a  , b , pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKWEeGnnTTKV",
        "outputId": "a99046d8-0402-4775-a79b-30ecb4b6f17d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('He spoke with such self confixence that his hearers coulx',\n",
              " '<start> He spoke with such self confidence that his hearers could',\n",
              " 'spoke spoke with such self confidence that the hearers thought ')"
            ]
          },
          "execution_count": 51,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = train.ERRONEOUS_SENTENCE[200]\n",
        "b =  train.english_inp[200]\n",
        "pred = inference(a,b)\n",
        "a  , b , pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4Lb_lfzq3KP",
        "outputId": "f4a971fd-b082-415f-ed74-47337f1164dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('The sovereigns Why y are sending ambassadors to compliment usurper',\n",
              " '<start> The sovereigns Why they are sending ambassadors to compliment the',\n",
              " 'negroes negroes they they are sending to to compliment ease honourable ')"
            ]
          },
          "execution_count": 52,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = train.ERRONEOUS_SENTENCE[300]\n",
        "b =  train.english_inp[300]\n",
        "pred = inference(a,b)\n",
        "a  , b , pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdvQD-7Dq2-V",
        "outputId": "1784e330-1359-461a-bc1f-138d9bce7384"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('What is it you are afraid of Lise I don',\n",
              " '<start> What is it you are afraid of Lise I don',\n",
              " 'is is it you are afraid of don don don ')"
            ]
          },
          "execution_count": 53,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = train.ERRONEOUS_SENTENCE[500]\n",
        "b =  train.english_inp[500]\n",
        "pred = inference(a,b)\n",
        "a  , b , pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxQ7BDQsq2wo",
        "outputId": "b324848f-aa9a-47dd-d74d-af817e3155e9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Secrets indeed All have secrets ff their fwn answered Natasha',\n",
              " '<start> Secrets indeed All have secrets of their own answered Natasha',\n",
              " 'indeed indeed you you your of their own answered ')"
            ]
          },
          "execution_count": 54,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = train.ERRONEOUS_SENTENCE[1000]\n",
        "b =  train.english_inp[1000]\n",
        "pred = inference(a,b)\n",
        "a  , b , pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txAQpSyVThEs",
        "outputId": "8d3eca47-19f3-48c2-f01b-44cebc1f9327"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:39<00:00, 25.10it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['do do do do think a favorite ',\n",
              " \"tell tell tell me that it 's on the way but i way is not \",\n",
              " 'a a a in school school ',\n",
              " 'but the weather off cold cold ',\n",
              " 'they they to talk to english english english ',\n",
              " 'a a of is is is been to the the ',\n",
              " 'why was the the the earthquake of the earthquake ',\n",
              " 'i i to do my husband ',\n",
              " 'it it the problem about the world ',\n",
              " 'the the is is is ',\n",
              " 'you you can you you you to to like also to this very ',\n",
              " 'was was i i i me ',\n",
              " 'i i years year old ',\n",
              " 'i i i tired ',\n",
              " \"did did n't know know how to know the job and it to 's \",\n",
              " 'i i i me so ',\n",
              " 'it it really sad ',\n",
              " 'the the is really nice ',\n",
              " 'i i trying to write a diary for in write ',\n",
              " 'there there there is a to me me me ',\n",
              " 'today today was the last lesson with my english english was was ',\n",
              " \"do do n't have to have \",\n",
              " 'it it a to be ',\n",
              " 'going going from university university i i i to study abroad ',\n",
              " 'is is the event of the world of in ',\n",
              " 'nobody nobody that that because because of people are the other house in the own ',\n",
              " 'the the the most as as a ',\n",
              " 'am am getting a than i see it it before i i ',\n",
              " 'my my my teacher asked to my friend and and he said something it voice voice ',\n",
              " 'you you can use to to or or other other a while ',\n",
              " 'my my new pc pc pc pc pc on this site was was very very comfortable ',\n",
              " 'the the will be to in the world world ',\n",
              " '',\n",
              " 'it is my dream ',\n",
              " 'the the are some of ',\n",
              " 'i i i study english english japanese japanese japanese ',\n",
              " 'we we have to go to a and though it have ',\n",
              " 'a a few days just to me me ',\n",
              " 'is is going to be a hot day ',\n",
              " 'the the in my room will be a big in the future ',\n",
              " 'of of all of of can use a same of to to ',\n",
              " 'go go go home and eat him eat ',\n",
              " 'the the other hand other other people have have a real life ',\n",
              " 'can can to a time the long time the time ',\n",
              " 'the the room room ',\n",
              " 'even even if if we talk about it it the time time time do ',\n",
              " 'now i i have have hours to the a ',\n",
              " 'a a has been ',\n",
              " 'the the the the interesting interesting ',\n",
              " 'also also is a for the because because because because was a hot in the on ',\n",
              " 'that that a lot ',\n",
              " 'we we we gave up and got them ',\n",
              " 'was was a nice time ',\n",
              " 'here here here here here here here here here ',\n",
              " 'the the the have it ',\n",
              " 'the the air area is in city in tokyo ',\n",
              " \"so so so busy because days i i are n't many \",\n",
              " 'he he has a hard and have has his and his wife ',\n",
              " \"if if we can our way we we can n't work to \",\n",
              " 'shopping shopping new new job job site site ',\n",
              " 'about about about minutes ago ago my house ',\n",
              " '',\n",
              " 'should should should i to it i is the week next week ',\n",
              " 'he he was to the floor car ',\n",
              " 'i i i to buy a book called a ',\n",
              " 'sometimes sometimes i i and i i a of his parents boyfriend ',\n",
              " \"did did n't eat it at it it of \",\n",
              " 'next next to the the the are using foreign of information and ',\n",
              " 'tell tell me if things things you have ',\n",
              " 'i i he will up of her to some that what i i that it ',\n",
              " \"it 's nice for me \",\n",
              " 'is is my first time writing this this diary ',\n",
              " 'sorry sorry sorry to him but i was ',\n",
              " 'speaking speaking speaking speaking english in the english in the school ',\n",
              " '',\n",
              " 'just just just one of to the the the the yen ',\n",
              " 'it it it ',\n",
              " 'i i i i to the the the ',\n",
              " '',\n",
              " 'went went went went shopping a restaurant restaurant and ate lunch ',\n",
              " 'time time to ',\n",
              " 'i i going to write my diary and listening ',\n",
              " 'have have a than a ',\n",
              " 'my my me life life life ',\n",
              " 'does does the big of of of of the nuclear nuclear of by ',\n",
              " 'we we were surprised there there of people ',\n",
              " 'to to to it it it for this site ',\n",
              " 'i i it ',\n",
              " 'every every day has has been and it a raining and hot ',\n",
              " 'to to to to get my mind ',\n",
              " 'my my is my my my ',\n",
              " 'can can can anyone some pictures money for the way time ',\n",
              " \"the the the people people are n't have one things \",\n",
              " 'know know know the the meaning ',\n",
              " 'on on the the other from in on the the the the ',\n",
              " 'today today today ',\n",
              " 'is is there we we we can eat other them ',\n",
              " 'from from my my my i i to to me ',\n",
              " 'live live in the middle middle in a big city city ',\n",
              " 'we we we we we we we ',\n",
              " 'so so so so not good at speaking and and speaking ',\n",
              " 'there are beautiful beautiful places ',\n",
              " 'someone someone be to lot of money to take the new this week ',\n",
              " 'it it minutes to the my office ',\n",
              " 'is is very much ',\n",
              " 'want want to talk with a lot of people ',\n",
              " 'we we are two years old people people who us and not of us ',\n",
              " 'because because the because the was a ',\n",
              " 'have have to things to write in this this my life life ',\n",
              " \"my my father does does n't like a rice a rice rice \",\n",
              " 'there there many many times ',\n",
              " 'teacher teacher teacher told me to me of to to to to find ',\n",
              " 'not not not really but i i i i just to to ',\n",
              " '',\n",
              " 'this this this time flies flies by with the other ',\n",
              " 'we we there the the have a days a the ',\n",
              " 'i i also a a big in the sky ',\n",
              " 'to to decided to make a friend to study ',\n",
              " 'love love to a today today very very happy and and i i english english ',\n",
              " 'their their their in are the are ',\n",
              " 'can can you on a me ',\n",
              " '',\n",
              " 'will will be at the month hours at next morning ',\n",
              " 'i i i hard hard hard and this to it is is be true ',\n",
              " 'ate ate delicious delicious food for the at the at the the the the ',\n",
              " 'i i i i not not been a room ',\n",
              " \"you you you that he own are n't to like him \",\n",
              " 'suddenly suddenly suddenly out times away ',\n",
              " 'most most thing happened had to and i ',\n",
              " 'i i me and and and and i ',\n",
              " 'it it writing on diary ',\n",
              " 'the the the reason thing is that the most is the year ',\n",
              " 'every every time i i i up today ',\n",
              " \"do do do n't like to be a the job to be it job \",\n",
              " 'usually usually we for a for a ',\n",
              " 'it it it to to see his voice and and he was he his own in the sea ',\n",
              " 'also also enjoyed the game game game the the the the ',\n",
              " 'will will our other ',\n",
              " 'in my life ',\n",
              " 'three three days my school has been raining in in of the vacation ',\n",
              " 'i i been working at i saturday saturday i i going to work tomorrow ',\n",
              " 'have have have a a little bit to the the ',\n",
              " \"could could could n't help me my my poor \",\n",
              " 'i i this new on this this morning ',\n",
              " 'is is a summer day ',\n",
              " 'my my my first time i a hour from my high ',\n",
              " 'two ago and and couple and and the river and the and the had ',\n",
              " 'is is located in the city area of the the the ',\n",
              " 'the the are some of of books and children and a a and and ',\n",
              " 'or or we them lot who who that thing for the i i ',\n",
              " 'it it it but it it it ',\n",
              " 'have have have was been hot ',\n",
              " 'is is a good for my first ',\n",
              " 'my my house for to school house in school ',\n",
              " 'had had a hair hair and a a small and ',\n",
              " 'many many many students students to to study to improve you my future way ',\n",
              " \"to to for the of my problem 's of not \",\n",
              " 'looking looking looking forward to seeing the movie and beautiful places in ',\n",
              " 'the the other for friend to ',\n",
              " 'how how do do do how how ',\n",
              " 'it it the ',\n",
              " 'it it it is easy to difficult ',\n",
              " 'finally finally on last last yesterday days days days ',\n",
              " 'is is it because be because i i been been watching ',\n",
              " 'my my and and and ',\n",
              " 'the the has very good in the ',\n",
              " '',\n",
              " 'should should should be more fun ',\n",
              " 'a a a a a friend friend a of and a friend ',\n",
              " 'have have to go up up at the house ',\n",
              " 'we we we we went to the the a a restaurant restaurant ',\n",
              " 'the the is is the the sea sea sea is called beautiful by a game ',\n",
              " 'also also to the the the hospital hospital and and this ',\n",
              " 'about about for for for about my my my diary ',\n",
              " 'i i to learn about for ',\n",
              " 'we we we went to the the the ',\n",
              " 'i i i do english english english much much ',\n",
              " '',\n",
              " 'but this is the same in in in changed the mind ',\n",
              " 'i i glad to i i ',\n",
              " 'i i to improve my english english i i to get a new class and a ',\n",
              " 'the the was was was to a american with a friend ',\n",
              " 'so i the reading reading reading reading read the new on on read ',\n",
              " 'in in last we we had a half to the the family home ',\n",
              " 'i i to be to to the the american american in in in ',\n",
              " 'see see see see the before ',\n",
              " 'bought bought a a ',\n",
              " \"most most of of n't know what it is \",\n",
              " 'the the the the us us there there there there are many people there ',\n",
              " 'so so in university and i the future i i be a university in now ',\n",
              " 'would would you go him him you you back home home home ',\n",
              " 'last last last last last a article site i i the internet i i ',\n",
              " 'have have the best time to the month ',\n",
              " 'here here for for a a years old old ',\n",
              " 'some some friend friends friends and and and me to help ',\n",
              " 'like like this summer than winter ',\n",
              " 'the the and and the me at i i waiting to be ',\n",
              " 'but but but teacher say very much ',\n",
              " 'was was was was was in on in the same same same same the the the the the ',\n",
              " 'i i happy to i i see that things them ',\n",
              " 'you you know you ',\n",
              " 'am am looking about this ',\n",
              " 'just just was the was was to the the of of we we be the ',\n",
              " '',\n",
              " 'the the of the are in high school china china china ',\n",
              " 'to to to to study ',\n",
              " 'the the i i i the on my but but i on on some on the the the the ',\n",
              " 'never never never forget the pain ',\n",
              " \"my my for have a long for my friend friend in we we n't have a friend \",\n",
              " 'is is because because i i to learn japanese ',\n",
              " 'i i ',\n",
              " \"i i i i i n't have a same that do \",\n",
              " 'help help me me me ',\n",
              " 'the the sky was very very and there of were lost ',\n",
              " 'a a happened last night ',\n",
              " 'fortunately fortunately fortunately i i i the i much i i friends there there ',\n",
              " 'you you are been in ',\n",
              " 'another another working for hours and day and i the ',\n",
              " 'there there of to take the bus of to i i for in a hours ',\n",
              " 'the the were talking ',\n",
              " 'to to can can to do it english but english english english it is very difficult ',\n",
              " 'is is like a a ',\n",
              " 'to to to to to be honest me i name name good ',\n",
              " 'finally asked asked her ',\n",
              " 'my my my friends in in a ',\n",
              " 'the the the the movie the i i stomach were tired ',\n",
              " 'say say the was was was the surprised to see them ',\n",
              " \"it it it sad if if if if as do but i i n't buy it water \",\n",
              " 'so so matter i if me feel that it it be a good thing ',\n",
              " 'even even now now now now going to get the cold cold ',\n",
              " \"for for for 's friend for a job for my my home my home yesterday \",\n",
              " 'do do my life will be it me way ',\n",
              " 'have have to study english day of my first ',\n",
              " 'i i i you you you you you you will to time ',\n",
              " 'no no no one of my diary ',\n",
              " 'i i try it again tomorrow ',\n",
              " 'so we had a lot time and talked about people things ',\n",
              " 'first first first first ',\n",
              " 'am am this this site on my pc pc my pc camera on phone ',\n",
              " 'to to write some some words ',\n",
              " 'it it it be a a same time in this this this ',\n",
              " 'go go going to school ',\n",
              " 'a a my friend and house we we we and and me and went went ',\n",
              " 'decided decided to keep this write in diary ',\n",
              " 'days days before yesterday i i was a big place for the health ',\n",
              " 'can can you time time when them at of the time ',\n",
              " 'also also also also have to summer holiday ',\n",
              " 'two two family parents gave his parents and and are married strong and ',\n",
              " 'i i we can have good good ',\n",
              " 'e e book mail to to read to book and buy and ',\n",
              " 'have have a big the in the middle game ',\n",
              " 'at at the time time i year i i the is is ',\n",
              " 'the the of the world is in in in ',\n",
              " 'was was not able to find a in a dictionary ',\n",
              " 'so so a man the the ',\n",
              " 'the the day time he he took a to a restaurant ',\n",
              " \"the ' now now now my my my working vacation grade in in the year of the the the \",\n",
              " 'are are many things that have have already have ',\n",
              " 'i i going to enjoy this this this summer vacation ',\n",
              " 'it it week day was in in a small store ',\n",
              " 'these these days are are are come up early and than before before ',\n",
              " 'it it it it just just since days first month days ',\n",
              " 'talking talking with her a a few years ',\n",
              " 'sometimes sometimes i to my my my trip ',\n",
              " 'i i want to go to a international trip ',\n",
              " \"my my 's my \",\n",
              " 'use use the computer ',\n",
              " 'also also not not be it but but but not first first are not bad bad ',\n",
              " 'we we were decided and and decided to read reading books books ',\n",
              " 'my my my is not so fun ',\n",
              " 'can can you your use ',\n",
              " 'i i to learn more languages languages ',\n",
              " 'i i a student who met the first first in the the the ',\n",
              " 'think think think is is a very kind person ',\n",
              " 'come come back and than me me me me me and she told me ',\n",
              " 'a a person i i home i the when when ',\n",
              " 'so so so sad because because he is me ',\n",
              " 'always always always said it it to to the the the the the the ',\n",
              " 'her her her mother time she her life ',\n",
              " 'the the time of time ',\n",
              " 'the the test is a test test ',\n",
              " \"would would n't like to play with it who who is a lot \",\n",
              " 'i i to get care of the ',\n",
              " 'because he had been been about ',\n",
              " 'is the best place of the world ',\n",
              " 'we we talked talked talked to a new student ',\n",
              " 'watching watching tv games games games ',\n",
              " 'it it it it it taste taste is delicious ',\n",
              " 'you you know speakers you you how you know what lot way or to learn ',\n",
              " 'people in the of the company of the company in in the apartment future ',\n",
              " 'why why a a is a very very who english a ',\n",
              " 'it it happy if someone can a able by by than ',\n",
              " 'my my poor and speaking and i and and i i today today ',\n",
              " 'there there there matter that it there there a more time time time have more lot time ',\n",
              " 'i i the the ',\n",
              " 'would would like to work for and a a time to my with my family ',\n",
              " 'like like the the country ',\n",
              " 'but but but so it ',\n",
              " 'a a hot spring will us us us come to ',\n",
              " 'what what what what do you think about it ',\n",
              " 'sometimes sometimes sometimes they they they are to different ',\n",
              " \"that that that that i i n't i i i it it \",\n",
              " \"no no many a few minutes before the of so so i i n't know \",\n",
              " 'go go go back to college of i my abroad i i i my university ',\n",
              " 'he he he family family ',\n",
              " 'it it it it a lot of time and and went dinner dinner ',\n",
              " \"could could could n't do it to to to n't understand the difference of the other \",\n",
              " 'enjoy enjoy reading books ',\n",
              " 'my my from my came in my my my my my family in been in of her ',\n",
              " '',\n",
              " 'is is very very popular in in ',\n",
              " 'i i a big fan of the the the the the ',\n",
              " 'my my my friend in my room ',\n",
              " 'people people of people are young ',\n",
              " 'you you ',\n",
              " 'going going going to take a first exam exam ',\n",
              " 'the the a a good convenient to to buy with other and the the ',\n",
              " 'and and a a a lot of ',\n",
              " 'have have a lot ',\n",
              " 'and and you think the is is a adult ',\n",
              " 'but english speakers is very very difficult ',\n",
              " 'they they the the water of the ',\n",
              " '',\n",
              " 'even even what the that that many many have many many other of many people ',\n",
              " 'but but but so i ',\n",
              " 'it it it very sad that i i that you you you you ',\n",
              " 'lang lang lang lang lang ',\n",
              " 'a a of on the of the the ',\n",
              " 'but as as a result of of result was was not than than other from from ',\n",
              " '',\n",
              " '',\n",
              " 'the the problem thing is very to ',\n",
              " 'another another teacher wrote a good for for ',\n",
              " 'university university university i i studying abroad ',\n",
              " 'my my my my my ',\n",
              " 'so so many things just just just really enjoying this time for ',\n",
              " 'it it interesting ',\n",
              " 'want want want to talk about the world where the the is to the the ',\n",
              " 'a a things about out ',\n",
              " '',\n",
              " 'to to to the internet internet internet ',\n",
              " 'going going going to take a time time time ',\n",
              " 'yes yes yes it truth of of tell tell it ',\n",
              " 'is is a today ',\n",
              " 'went went to ',\n",
              " 'with with with by people of people people ',\n",
              " \"do do n't have any things to do it but i i i i i just \",\n",
              " 'the the cold night was very and and were and and me ',\n",
              " \"do do you you have n't have a \",\n",
              " 'so so looking looking for me who can help me with my english skills skills skills ',\n",
              " 'you you can you you know know you question thinking you you you want you you ',\n",
              " 'i i to the the the on year ',\n",
              " 'in in in japan two there there are many people ',\n",
              " 'who also have about her ',\n",
              " 'the the the daughter have been a cold day yesterday ',\n",
              " 'it it come here after a days day after have this ',\n",
              " 'so so so my i i i go to the countries and my my friends ',\n",
              " \"do do n't know why he me \",\n",
              " \"if if if you do that are are did i n't \",\n",
              " 'the to buy it ',\n",
              " 'the the is located in in the the the the ',\n",
              " 'was was very hot of of the rain ',\n",
              " \"could could was to be good at my myself but but i i n't be \",\n",
              " \"it it 's not learning foreign the in a a which in the the the \",\n",
              " 'one one day was was the by the the ',\n",
              " 'is is about a long ',\n",
              " 'but but but way ',\n",
              " 'can can use use more more more than and more ',\n",
              " 'to to to some of on the the i on the ',\n",
              " 'is is are very in the countries ',\n",
              " \"is is a new a a for lang of my friends ' ' \",\n",
              " \"he he ca n't go to school \",\n",
              " 'one one day day of my friend to meet meet meet a new friend ',\n",
              " 'now now home my home office of was was ',\n",
              " 'rice rice rice rice rice with it it with with ',\n",
              " 'i i a exam exam at on my office ',\n",
              " 'my my son will working at every work and work work ',\n",
              " 'to to to to a a a foreign ',\n",
              " 'at at my is the best for for me ',\n",
              " 'teacher teacher school in school school school has has has ',\n",
              " 'we we wanted to buy on the ',\n",
              " 'my my my friends will go to the the the ',\n",
              " 'there were many things things things ',\n",
              " 'the the sky is a out my job ',\n",
              " 'you you want to be a with me ',\n",
              " 'do do not have in big in the same ',\n",
              " 'my my friend and and and friend and and and we we go place place was the us us us ',\n",
              " '',\n",
              " 'i i ',\n",
              " 'some some songs when a a here the same in all or and you to to ',\n",
              " 'it it it was the it i i the the i i bus i up ',\n",
              " 'i i i i learn english english ',\n",
              " \"do do n't know if some books are in the the much much they we were talking talking \",\n",
              " \"time time do do n't go to the city \",\n",
              " 'was was was a high school student i i i to the park festival ',\n",
              " 'it it it tastes like or it to to is to to effective to the ',\n",
              " 'i i to take the the next day ',\n",
              " 'at at at that he he he he got back the door had had a best ',\n",
              " 'is is a nice ',\n",
              " 'the the power plant is ',\n",
              " 'i i to school today ',\n",
              " 'i i me ',\n",
              " \"of of 's 's 's nice to you for wait for me for a long \",\n",
              " 'have have a the one year old ',\n",
              " \"the the to to that that it 's not to be a best day of my life \",\n",
              " \"is is the is is is difficult but but i i is n't like what and and and \",\n",
              " 'about about about about the the the the company company ',\n",
              " 'finally finally finally home home home and i my little bit a face ',\n",
              " \"but i i n't a \",\n",
              " 'i i are be a of them ',\n",
              " \"you you you know a 's much like like he he it \",\n",
              " 'i i the the the the ',\n",
              " 'a a a a by by ',\n",
              " 'a a a email to a small salon yesterday ',\n",
              " 'he was watching for a while ',\n",
              " 'i i i going to speak english english ',\n",
              " '',\n",
              " \"one one one one one it it it 's good than my new workers in new \",\n",
              " 'in in all i i i i to get the on the job ',\n",
              " 'i i to the hot spring yesterday ',\n",
              " 'more more more more more ',\n",
              " 'the the is like like like like than used or before out of the ',\n",
              " 'this this will will will will be a long in days ',\n",
              " 'you you to a a phone with my ',\n",
              " \"it it it 's raining cold \",\n",
              " 'the the other hand how how you who of of their ',\n",
              " 'many many many a japanese japanese japanese and japanese japanese who japan japan japan ',\n",
              " 'is is a it bad dream ',\n",
              " \"lunch lunch and at 's held minutes hours hours hours \",\n",
              " 'no no one things things ',\n",
              " 'usually usually do most of the most in the life ',\n",
              " \"the the second of of of can can n't know \",\n",
              " 'a a result the the cost snow jams the earthquake conditioner ',\n",
              " 'i i learning english english learning learning learn ',\n",
              " 'is is the like like like the world like the the ',\n",
              " 'also also a with business and and and a ',\n",
              " 'i i to know some other and some some in new in in english school ',\n",
              " 'is is it because because of the the ',\n",
              " 'i i a ',\n",
              " 'this this this first diary diary diary entry entry ',\n",
              " 'i i was friday friday friday friday ',\n",
              " 'finally finally a long i i i i i a a a a a a little ',\n",
              " 'reading reading the book i said said first first ',\n",
              " 'my my of my room was in in the the the was earthquake was ',\n",
              " 'to to to to be ',\n",
              " '',\n",
              " 'does does not take a to lot of money to go to ',\n",
              " 'a a to the my today ',\n",
              " 'when when i you came me ',\n",
              " 'i i very happy to go back home ',\n",
              " 'had had a little tired i i i husband ',\n",
              " 'so so my my mom friend when i i on the of the bus ',\n",
              " 'her her bought a a and ',\n",
              " \"ca ca n't imagine the difference of of the of the \",\n",
              " 'just just just just watching the movie movie movie ',\n",
              " 'nice nice be you ',\n",
              " 'is is my diary diary ',\n",
              " 'it it time time time time to study ',\n",
              " 'i i surprised to hear that ',\n",
              " 'is is the first time to ',\n",
              " \"do do do n't like a friends speakers \",\n",
              " \"what 's the difference between them \",\n",
              " \"i i to the in in the 's a office office today \",\n",
              " 'going going going to go to a a class ',\n",
              " 'can can help me to order mind ',\n",
              " \"my my my father 's his is one best one for our time \",\n",
              " 'to to to be honest we we want to to go to same ',\n",
              " 'have have a a since last week week week ',\n",
              " 'i i i happy today ',\n",
              " 'i i studying english english hard ',\n",
              " 'work work work the the ',\n",
              " 'you you you you know know you if i like it it it ',\n",
              " \"is is the difference between ' ' and ' ' ' ' \",\n",
              " 'have have been almost almost since we we here ',\n",
              " 'actually actually actually actually actually actually actually want to improve my english english ',\n",
              " 'i i writing and and japan ',\n",
              " 'can can see you love like ',\n",
              " 'like like that the which which the the the a old to live and the ',\n",
              " 'is is it to be the sentence ',\n",
              " \"suddenly suddenly a little and he and and him but i does n't care it \",\n",
              " 'think think the difference culture are the difference of the ',\n",
              " 'her her her husband was forward happy to the afternoon ',\n",
              " 'the the were in ',\n",
              " 'i i not a adult ',\n",
              " \"does does n't use a lot \",\n",
              " 'i i minutes hours minutes the bus of the the water of the money ',\n",
              " 'especially especially especially i this this site is a good way for get up ',\n",
              " 'is is the same same same ',\n",
              " 'even even i i to learn to to i i to go to the ',\n",
              " 'this this have have have a few of homework i do it it it very tired ',\n",
              " \"that that 's made made made a lot to i i at \",\n",
              " 'god god god me me not matter to me but it it not not to him ',\n",
              " 'finally finally a family and asked and and i i i my my my ',\n",
              " 'is is a beautiful beautiful city ',\n",
              " 'a a my my two months old daughter has me a big party festival ',\n",
              " 'the the was difficult to to be it to to to be ',\n",
              " 'is is there many things and a same same make a things in this same same same ',\n",
              " 'reading reading reading reading and i i to improve and and i and ',\n",
              " '',\n",
              " 'my my teacher my my friends and i i going to do ',\n",
              " \"it it 's the the was was on were home home \",\n",
              " 'i i that that that that that that ',\n",
              " 'i i ',\n",
              " \"the the may may n't do to the the the to the time \",\n",
              " 'at at at ',\n",
              " 'the the nuclear the the air the for our house for i i going at at ',\n",
              " 'my my wife has has have have a coffee ',\n",
              " \"have have n't seen the movies but i i i to watch the movie \",\n",
              " 'her her her ',\n",
              " 'when when i i a to this this it ',\n",
              " 'i i i to the it go go ',\n",
              " 'almost almost just almost just a a the watched before before ',\n",
              " 'have have some of which i i be been do you soon as possible possible over ',\n",
              " 'i i the the the same in the the the ',\n",
              " 'love love the and and and played ',\n",
              " 'there are many people and then ',\n",
              " 'i i i to say that that is is that of to that learning speakers ',\n",
              " \"it it it it this is the foreign where we do their order i do n't know \",\n",
              " \"have have n't n't time but in my my my \",\n",
              " 'talking talking talking with with with with friends friends and the job day tomorrow study ',\n",
              " 'you you have a ideas please please help me ',\n",
              " 'believe believe the the truth way i the the the site and and ',\n",
              " 'it it that i i to get up on my future future future future ',\n",
              " 'i i i to to the big experience to ',\n",
              " 'looking looking looking forward to go to to the the ',\n",
              " 'it it the the th ',\n",
              " 'even even when when i say to at home house ',\n",
              " 'it it a little bit because because i i not in to write in diary in english english english ',\n",
              " 'a a have changed as as a a a high high school ',\n",
              " \"sometimes sometimes i to tell him i but i 's not a to the \",\n",
              " 'we we there are a new new in the countries ',\n",
              " \"she she did n't she she she was able at time time \",\n",
              " 'the th started now ',\n",
              " 'i i to will to to ',\n",
              " 'have have it time time to a for a trip ',\n",
              " \"then then decided decided decided to take a new year to 's to me \",\n",
              " 'before before was to i i written my diary diary in the few few ',\n",
              " 'everybody everybody everybody the the has the power to eat ',\n",
              " \"do do do n't you me because because i i not to \",\n",
              " 'i i i was really tired it ',\n",
              " 'you you like the ',\n",
              " 'morning morning morning i i the big the in my of my house ',\n",
              " 'our our way to go going the restaurant store to go a ',\n",
              " 'felt felt happy a a a friend friend and family ',\n",
              " 'our our our friends will us to go ',\n",
              " 'is is a really ',\n",
              " 'so so so so in home because i i already day students so ',\n",
              " 'job job is ',\n",
              " 'and and and and ',\n",
              " \"sometimes sometimes let let up the company company the company country 's i i in \",\n",
              " 'the the the the are the important country in in in the area ',\n",
              " 'it it was to my first from from but i i a than than than ',\n",
              " 'the the ate said to talk able with the other family ',\n",
              " 'i i a friend to from a elementary elementary yesterday ',\n",
              " 'suddenly suddenly a after my room time has a the the a a a ',\n",
              " \"now now she does n't have a has has a cold throat \",\n",
              " 'and and and a second second at a tv shop shop ',\n",
              " 'i i to do that work as the the the company ',\n",
              " 'and and friend went went went to the to study ',\n",
              " 'sometimes sometimes seems to be kind and and sometimes ',\n",
              " \"it it it it it that 's me is bad bad \",\n",
              " 'i i how people are in the is now now ',\n",
              " 'the the of food are are has rice rice yen and and and and rice ',\n",
              " 'we we we we had not train conditioner ',\n",
              " 'and then to ',\n",
              " 'happy happy happy i i happy ',\n",
              " 'decided decided decided to study more than english english english ',\n",
              " 'she she her she her house she she mother to her house in in ',\n",
              " 'in in in the i i i to be it meaning of this site ',\n",
              " 'is is not a best reasons ',\n",
              " 'the the the the have have have a good good at least least least ',\n",
              " 'love love this sound ',\n",
              " 'what what what that my my improve my writing writing and ',\n",
              " 'day day of ',\n",
              " 'now now today from month from i i from school ',\n",
              " 'is is very important ',\n",
              " 'going going going to her her home ',\n",
              " 'the the is very difficult but but but i i that that be able to do the ',\n",
              " 'and and are are are very very different ',\n",
              " \"the the the the which are so in you do are n't get any \",\n",
              " 'just just too much to have in a a ',\n",
              " 'both both hands with them of the ',\n",
              " 'the the in first of years years ',\n",
              " \"so so i i i i i i i it 's not to to and time of going \",\n",
              " 'the the the was a ',\n",
              " 'is is a famous famous the place for our students ',\n",
              " 'will will will try to write in diary in english ',\n",
              " '',\n",
              " 'a a a a a for a interview trip for about ',\n",
              " 'like like the game player ',\n",
              " '',\n",
              " 'is is because because of of a been working late ',\n",
              " 'there there learned about interesting site today ',\n",
              " 'of of all of of of like to study japanese for ',\n",
              " 'most most most of the was the kind of the for japanese of the ',\n",
              " 'is is a long holiday in in in in the the be be the the the ',\n",
              " 'today today today day day ',\n",
              " 'so so young people said meet me ',\n",
              " 'i i lang lang lang ',\n",
              " 'a a a japanese of meet meet ',\n",
              " 'the the is ',\n",
              " 'more more more be more the more more we we than and be more ',\n",
              " 'my my body is is yen yen ',\n",
              " 'if if if if kind the difference of the to to to the the the part time of of ',\n",
              " 'and so i and they they be ',\n",
              " 'i i never we we are in was i i very hot ',\n",
              " 'suddenly suddenly suddenly suddenly suddenly a warm to the sun and the and and and the ',\n",
              " \"he he does n't have any lot of money and go to go abroad \",\n",
              " 'time time time writing good time writing i i ',\n",
              " 'i i i listening listening listening skills skills skills a book of and books ',\n",
              " 'please please new new life ',\n",
              " 'one one one old old who to my friend ',\n",
              " 'are are many a days these every days ',\n",
              " 'in in in a lesson of of friend told me to i i have him ',\n",
              " 'they they a of money and and a ',\n",
              " 'can can do do it ',\n",
              " 'to to the the new month i the hours ',\n",
              " 'both both them always always in the year year ',\n",
              " 'even even we we the the we was the lot thing between the and not ',\n",
              " 'you you you can see a cold cold break of a hot night ',\n",
              " 'i i to you ',\n",
              " \"even even 's not have have have has have been to you do \",\n",
              " \"could could n't say that lot \",\n",
              " 'was was a amazing big earthquake ',\n",
              " 'i i i to communicate with people people in the ',\n",
              " 'are are some people when the earthquake was was was was more of money ',\n",
              " 'so so too busy to least for take time to to study ',\n",
              " \"a a 's a a old english who who a teacher english \",\n",
              " 'my my my wife had a hour night and in the summer in in in is in close ',\n",
              " 'they they have ',\n",
              " 'what what it it is is very very cute ',\n",
              " 'my my friend said i i i in the the the ',\n",
              " \"it it already already it and i i n't go to to go hospital again \",\n",
              " \"do do n't understand \",\n",
              " 'was was about and the teacher of the and the job ',\n",
              " 'could could there one there was there one there there ',\n",
              " 'to to my my dream is be a best for for ',\n",
              " 'is is is you as a different as the of ',\n",
              " 'was was the a and and i i the ',\n",
              " 'to to to be the same situation and i know it it the ',\n",
              " 'the the a a year and played this song ',\n",
              " 'my my wife were gone ',\n",
              " 'it it it it it difficult to to same of the way ',\n",
              " 'the the the picture on the bus of lunch for a at the game ',\n",
              " 'my my my friend said to me ',\n",
              " 'i i going to meet you ',\n",
              " 'to of them ',\n",
              " 'it it been looking forward to this movie ',\n",
              " 'a a a japanese who lives in japan japan study study studying ',\n",
              " 'going going to the new in in the year days ',\n",
              " 'is is also as but i ',\n",
              " 'today today will will will write my in my diary ',\n",
              " 'the the the story scene which is been big amount of the ',\n",
              " 'there there be in ',\n",
              " 'apple apple wa de and de etc ',\n",
              " 'after after the i happened happened ',\n",
              " 'will will will try to write this in this site ',\n",
              " 'going going going to a library of meet some new in a few weeks ',\n",
              " 'i i to use more more ',\n",
              " 'many many many him he things to go him to go there ',\n",
              " 'the the is in far from from the i i now ',\n",
              " 'maybe maybe maybe he has is of the ',\n",
              " 'was was it tired i to hear ',\n",
              " 'people people who who me came came to me me birthday ',\n",
              " 'a a a a long time ago my my passed hair cut ',\n",
              " 'would would like to tell you about my in my house ',\n",
              " 'i i i try to keep this tomorrow ',\n",
              " 'wanted wanted to buy and and the the sound ',\n",
              " 'went went went to the restaurant restaurant and and went hours i i go restaurant restaurant ',\n",
              " 'like like eating too too i i also to be a hour for me first first first first ',\n",
              " 'perhaps perhaps the is one of the most in in japan ',\n",
              " 'which is very nice for me ',\n",
              " 'in in their people animals clothes are are nuclear family is a beautiful ',\n",
              " 'most most time to be to other languages in in i the use use a foreign ',\n",
              " 'is is is is very good and good a good ',\n",
              " 'almost almost here in japan world and and of of to watch of ',\n",
              " 'i i i i studied ',\n",
              " 'was was a about the the a to speak many things ',\n",
              " 'so many japanese my favorite team are coming to to to to japan japan city ',\n",
              " 'there there there there is a business trip ',\n",
              " 'it it too expensive ',\n",
              " 'that that that does not the same of of ',\n",
              " 'we believe that it is be the to the food ',\n",
              " \"think think of 's be popular to japanese country \",\n",
              " \"did did did n't my my bed i i i i too \",\n",
              " 'is is my favorite in in the future future future ',\n",
              " 'people people should people want want to be their own own ',\n",
              " 'is is there the the the is the famous famous place which which the the the ',\n",
              " \"come come back home her and i 's because and but but i it 's not \",\n",
              " 'like like like english english english ',\n",
              " 'my my friends ',\n",
              " 'i i i to work your hair time ',\n",
              " 'have have a little little this this and and and it it it ',\n",
              " 'the the of is is the the in the hot ',\n",
              " 'the the the the the the on the air ',\n",
              " 'there there there are many many ',\n",
              " 'i i the two i years ago i the up the the i was me to it ',\n",
              " \"do do do do n't idea how to do you how \",\n",
              " 'besides besides besides my my opinion in in a sentence in a ',\n",
              " 'i i i i it a cold ',\n",
              " 'the the a was a little bit difficult ',\n",
              " 'i i i studying english since in a day but but i i really bad bad ',\n",
              " 'then then to the the the the the day day ',\n",
              " 'now now it it studying in ',\n",
              " 'was was was very good ',\n",
              " 'the the my my my boss ',\n",
              " 'my my my friends who is to about about for ',\n",
              " 'today was was up the my and my sore throat ',\n",
              " 'i i study ',\n",
              " 'was was a long day ',\n",
              " 'the the will think think that will will a experience to go our ',\n",
              " 'finally finally to the library family ',\n",
              " 'the the found that of on the and the the own ',\n",
              " 'the the ',\n",
              " \"i i i to do i i n't like to do do \",\n",
              " 'my my my wife took took a bus to a walk to to a lot ',\n",
              " 'most most most people would like best and and we we be to make a native and in ',\n",
              " 'can can can you about the a ',\n",
              " 'someone someone someone who is a favorite who his and and is a by of and his ',\n",
              " 'so so so happy that that that that some of friends in all over the world ',\n",
              " 'the the are very beautiful ',\n",
              " 'it it it ',\n",
              " 'when when when when i i home home i i i will on the time time time ',\n",
              " 'it it it is a little and and ',\n",
              " 'he he he he did not work me ',\n",
              " 'i i happy to write a a my my my ',\n",
              " 'then then their own and a year years and a a ',\n",
              " 'can can you the book of the way and i sound is ',\n",
              " 'have have me have dream is is first of of years years old ',\n",
              " 'my my my friends i to study english english a is a ',\n",
              " 'people people said said said said that bad has a in the ',\n",
              " \"have have n't been here it for me long time \",\n",
              " 'is is my first diary ',\n",
              " 'the the the of of of the weather weather ',\n",
              " 'working working will will keep a diary ',\n",
              " 'today today today today in a new year ',\n",
              " 'i i to to me to me ',\n",
              " 'my my my my hometown is very ',\n",
              " 'the the a a celsius celsius a a a ',\n",
              " 'the the the has the the a same style ',\n",
              " 'oh oh what would i i to be a new year year ',\n",
              " 'but but but but but but than winter ',\n",
              " 'i i my to my my room and and much much to it ',\n",
              " 'the the the team played the game game team the city team team the the game game game of of ',\n",
              " 'was was the of the ',\n",
              " 'to to so far ',\n",
              " 'wait wait for the see the next time ',\n",
              " 'my my my first just changed the computer driver for for a new job ',\n",
              " 'it it so good ',\n",
              " 'and and and food and ',\n",
              " 'now now the the the the big plant in of the earthquake earthquake ',\n",
              " \"the the was he i i it i i n't in my morning today night \",\n",
              " 'the the the train i out the the way and body ',\n",
              " 'you you you ',\n",
              " 'should should they play to be with with i ',\n",
              " 'can can see the cherry beautiful cherry ',\n",
              " 'the the is very happy ',\n",
              " 'so so so so so so so i i not write to write my diary ',\n",
              " \"do do n't feel tired but but i i to this at this this summer year \",\n",
              " 'her her like when when when he was it for for ',\n",
              " 'is is it to me to hear his story ',\n",
              " 'suddenly suddenly a water ',\n",
              " 'never never put it ',\n",
              " 'i i my friends can order speaking speaking help me ',\n",
              " 'since since when when today i i it it be it and i i ',\n",
              " 'do do i to ',\n",
              " 'is is it important for with for us ',\n",
              " 'it it it it been really it busy at my school ',\n",
              " 'in in in in in the of the of last last ',\n",
              " 'i i like we to played in in in in japan ',\n",
              " 'to to with english english listening with speak english ',\n",
              " 'they they they they their from the ',\n",
              " 'are are very much ',\n",
              " 'usually usually you you know my my japanese has many reasons ',\n",
              " 'it it it be been sleep as as as as as diary ',\n",
              " 'i i looking forward to going there ',\n",
              " 'two two two years years old the the the company ',\n",
              " 'now now university university student in in japan ',\n",
              " 'we we a lot of things things ',\n",
              " 'a a a in my hometown ',\n",
              " 'everyone everyone are watching movies and watch ',\n",
              " 'is is very much ',\n",
              " 'it it is a great thing ',\n",
              " 'is is is the ',\n",
              " \"the the did do n't get on the end in the while \",\n",
              " 'no no no only only is only a lot place of the the to to to to be ',\n",
              " 'i i to tell my feelings to the the see other ',\n",
              " 'they they the the the live the and and the ',\n",
              " 'it it very little bit that kind kind of the is be used to the ',\n",
              " 'a a a girl girl who is learning and and ',\n",
              " 'do do you think of your time time ',\n",
              " 'my my my job at ',\n",
              " 'i i like a movie ',\n",
              " 'the the of them them to to look a movie movie ',\n",
              " 'i i i start to on now on ',\n",
              " '',\n",
              " 'i i i i i i friend and and make ',\n",
              " 'every every the the the has been days and it body is to ',\n",
              " \"ca ca n't wait my job \",\n",
              " 'many many many many people who ',\n",
              " 'is is is is is playing tennis ',\n",
              " 'a people to people people people ',\n",
              " 'it it a long day last week ',\n",
              " 'the the about yen yen ',\n",
              " 'moreover moreover when when a a a good book about the the ',\n",
              " 'went went went went to the school school and car and took ',\n",
              " 'so so not sure but but it it of ',\n",
              " \"do do n't let know this this this work this time \",\n",
              " 'the the the the you feel the at the end ',\n",
              " 'my my his my family is very a for for ',\n",
              " 'do do not do what what it it be it ',\n",
              " 'friend friend and and went went to the morning at morning ',\n",
              " 'my my came from from my university trip today ',\n",
              " 'is is is is a weather ',\n",
              " \"it it important thing thing n't it \",\n",
              " 'did did did i going to school for the at the morning ',\n",
              " 'speaking speaking has has has a a a lot of the job of the ',\n",
              " 'i i very about for the other for about about i i i at the office ',\n",
              " 'there there there there there been in the the on the internet ',\n",
              " 'i i it hot and and i i too tired ',\n",
              " 'if if if the is students are in held in the countries ',\n",
              " 'today today today hometown today today today today old now is is is old old be years years ',\n",
              " 'so wanted wanted to look so so so so so so so so tired ',\n",
              " 'is is a very beautiful place for i all month month be to the more the ',\n",
              " 'i i me to and and and and be like it ',\n",
              " 'should should should should you to a my diary ',\n",
              " 'suddenly suddenly the as the second second of the world in in the ',\n",
              " 'was was the a way and the ',\n",
              " 'like like like ',\n",
              " \"what what what 's 's \",\n",
              " 'what what what that japanese japanese think japanese and and and japanese japanese japanese ',\n",
              " 'making making making to the a money to of money ',\n",
              " 'i i a of my last last last night night ',\n",
              " \"did did n't you that they have the in a in japanese people \",\n",
              " 'also also also also still long for keep my hair on ',\n",
              " 'i i i her felt felt felt very happy ',\n",
              " 'writing writing writing my diary plan to to i you you to about it to write ',\n",
              " 'decided decided decided to buy a new pair pair ',\n",
              " 'also also also put from of the ',\n",
              " 'her her and her she she was married and and the the in the hospital ',\n",
              " \"some some some of some of the students to you 's them them \",\n",
              " 'what what that people were going from with ',\n",
              " 'i going going and the we we to go to japan japan japan ',\n",
              " 'have have may may have a days than days ',\n",
              " 'a a a my my my my and my family ',\n",
              " 'i i the to the the the to get ',\n",
              " \"do do getting whether they they it 's not as as well well \",\n",
              " 'they they did not been the little of of of very ',\n",
              " 'and and the world and and ',\n",
              " 'want want want want to to to go to the new year ',\n",
              " 'i i i i i how the person way the way ',\n",
              " 'i i to get to for for ',\n",
              " 'also also also the very and and it was hard to me to buy a ',\n",
              " 'next next next year years ago and went restaurant and and so ',\n",
              " 'i i my friend on the th of th ',\n",
              " 'the the and the ',\n",
              " \"they they they could n't see a movie of she had a very \",\n",
              " 'of of their friends friends friends ',\n",
              " 'the the the first was not good good ',\n",
              " 'i i to take a bath and a a food ',\n",
              " 'one one help me please please please please ',\n",
              " 'the the kind like what kind kind do you you you ',\n",
              " 'like like this song of this song ',\n",
              " 'if if you think the things ',\n",
              " 'on on at the end end the the the ',\n",
              " 'not not not not not that it it not good good way way ',\n",
              " 'like like to the difference of what ',\n",
              " 'hello hello hello ',\n",
              " 'it it in in the countries ',\n",
              " 'is is there to us to go there to we can to study our our place ',\n",
              " 'many many many many people ',\n",
              " \"they they got the that the what what ca n't be it it \",\n",
              " 'the the of the difference seems to be that that that we us is like we have ',\n",
              " 'all all on the the way to to change dream ',\n",
              " 'a a a a a a cold break ',\n",
              " 'that that that would be able to be our of for the next of the month ',\n",
              " 'sorry for any reason ',\n",
              " 'it it that about this place ',\n",
              " 'a a a a have like a ',\n",
              " 'can can use the site ',\n",
              " 'ate ate eating and played a lot of a ',\n",
              " 'the the that that i me that that most that she was a ',\n",
              " 'also also to the to to to to be a job and is a a ',\n",
              " 'my my give me a message or make make to use the company in the future ',\n",
              " 'the the a picture of the tea ate rice are ',\n",
              " 'it it story story that that ',\n",
              " 'have have have a new class in two weeks ',\n",
              " 'have have to go to work at the the day after tomorrow ',\n",
              " 'the the is a a a ',\n",
              " '',\n",
              " 'a a hour to to to took to of the school ',\n",
              " 'on on on on in diary in my life life ',\n",
              " 'is is is is name is ',\n",
              " \"it it 's going to go a trip of work to get my future \",\n",
              " 'i i go him at saturday saturday saturday ',\n",
              " 'it it it it it are so hot and drink a a ',\n",
              " 'a a a man who is learning ',\n",
              " 'to to go next the next than year ',\n",
              " 'the the was so popular from the same of the most most ',\n",
              " 'just just just i the the i i been finished to watch ',\n",
              " 'especially especially especially summer would would this this will will will ',\n",
              " 'thank thank thank thank thank thank thank ',\n",
              " 'i i to improve my english ',\n",
              " 'it it out that that ',\n",
              " 'usually usually usually usually used to the ',\n",
              " \"it it it it is n't when a i i a while \",\n",
              " 'love love ',\n",
              " \"suddenly suddenly pm pm i i to the country but not not n't be any \",\n",
              " 'yes yes it was not by on on on on from the train train train ',\n",
              " 'they they they said that that that that that that that that can be us ',\n",
              " 'am am at am was i a bicycle ',\n",
              " 'thanks thanks everyone ',\n",
              " 'the the way went went went to places ',\n",
              " 'will will be a of of are are verbs etc etc etc ',\n",
              " 'are are there lot of the more more on ',\n",
              " 'a a the hot and a for the hot ',\n",
              " 'has has been been raining for two months ',\n",
              " \"the the problem is not i i n't use in in use as i use n't use with with with \",\n",
              " 'a a a busy for a a a ',\n",
              " 'one one one thing is is to say ',\n",
              " 'the the my my boss score will do be able to my home ',\n",
              " 'people people people to person person with children to teach each each other in school school ',\n",
              " 'suddenly suddenly suddenly off my my my up and my house and went went to a bus ',\n",
              " 'if if and a have more and and are use more foreign country and and and other ',\n",
              " 'my my you you enough time ',\n",
              " \"or do you you to use in in the ' the \",\n",
              " 'could could i i were all all all all ',\n",
              " 'the the the the the class class i i i some books in in english english english ',\n",
              " 'my my a day day and i one came not ',\n",
              " '',\n",
              " 'i i say that that that that ',\n",
              " 'we we we we made a rice soup and a small and ',\n",
              " \"i i i i i him to know it 's i is is very important \",\n",
              " 'there there there there people could could be able in eat ',\n",
              " 'my my to introduce you ',\n",
              " 'my my my friend at the other of and was the and of first was a ',\n",
              " 'the the the the one one was was was one one of of ',\n",
              " 'the the picture of was the bus clock ',\n",
              " 'will will will take a test to study to english ',\n",
              " 'are are very good ',\n",
              " 'the the we we we food food for the the the the by by them ',\n",
              " \"i i just this this 's \",\n",
              " 'is is in in ',\n",
              " 'here here will will will write your diary diary for this life years ',\n",
              " 'the the the are a lot place of of it have have have ',\n",
              " 'a a a lot of people and we able to make a ',\n",
              " 'a a a little bit to to watch a a time as the ',\n",
              " 'can can people people but but the same was so difficult to to to ',\n",
              " '',\n",
              " 'the the other many many people know that has been ',\n",
              " 'and and the and and hot ',\n",
              " \"i i up and i to he he my my friend and my family 's house phone \",\n",
              " 'will will be a in a big ',\n",
              " \"this this that that 's 's is not do that that going the home \",\n",
              " 'you you you you you you ',\n",
              " 'what what happened the the the the the a earthquake year i recently recently recently now ',\n",
              " 'second second i with to the difference of the and and ',\n",
              " 'to to to learn english english english english english difficult and difficult ',\n",
              " \"can can enjoy it with 's a long time to to body \",\n",
              " \"it it feel feel like that 's like kind life \",\n",
              " 'did did did did going going to go to my school trip ',\n",
              " 'to to to to the the the the the today day time ',\n",
              " 'thanks thanks my my diary ',\n",
              " 'is is is coming minutes pm ',\n",
              " 'the the the the the the famous ',\n",
              " 'i i i my friend and teach my and skills ',\n",
              " \"that that 's that that is a big air \",\n",
              " 'it it to to be it hard ',\n",
              " 'for for a first my a a a a a big bit the the the way ',\n",
              " 'one one there one of my friends friends who some problems and her ',\n",
              " 'going going going going to a time time time time and play for a job ',\n",
              " 'have have have their and and his and mind ',\n",
              " 'yesterday yesterday yesterday ',\n",
              " \"also also do n't like it \",\n",
              " 'one one one of the most famous in in the world ',\n",
              " 'so so to my my i was so so big that we are no big people ',\n",
              " 'not not not written in diary in english english english ',\n",
              " 'it it in was very very of of of of very ',\n",
              " 'help help advice me me me me ',\n",
              " 'the am am a with a very man ',\n",
              " 'learning learning learning languages skills english english english ',\n",
              " 'i i ',\n",
              " 'the the the the a to a a a ',\n",
              " 'it it to a lot ',\n",
              " 'the the of of your and are are your life is ',\n",
              " \"my my dream think is is seems because it it 's difficult easy lot \",\n",
              " 'a a diary in a a a a ',\n",
              " 'the the the a ',\n",
              " 'more more more more more best important important my english skills ',\n",
              " 'the the was the a ',\n",
              " 'is is is the in the ',\n",
              " 'is is time time time ',\n",
              " 'also also also in in the the korea korea studying abroad ',\n",
              " 'think think it was a little bit bit when the end ',\n",
              " 'it it it it that was to and said ',\n",
              " 'our our our company is is now ',\n",
              " 'a a a new of a and and i a a lot ',\n",
              " \"a a my english english english is my so i i n't have i to \",\n",
              " '',\n",
              " 'to to with our with with my friends friends ']"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_train = train.sample(1000)\n",
        "result = []\n",
        "i=0\n",
        "for i, j , k in tqdm(sample_train.values):\n",
        "    # print(i , j , k)\n",
        "    pred = inference(i , j)\n",
        "    result.append(pred)\n",
        "\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 907
        },
        "id": "uMFeNbysHjWI",
        "outputId": "8711c005-8d19-4b61-ea68-cf093d5f1919"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>incorrect</th>\n",
              "      <th>english_inp</th>\n",
              "      <th>english_out</th>\n",
              "      <th>PREDICTED_SENTENCE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14235</th>\n",
              "      <td>So , I must think fashion sense .</td>\n",
              "      <td>&lt;start&gt; So , I must develop my fashion sense .</td>\n",
              "      <td>So , I must develop my fashion sense . &lt;end&gt;</td>\n",
              "      <td>do do do do think a favorite</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108585</th>\n",
              "      <td>This saying tell me that it depends on your he...</td>\n",
              "      <td>&lt;start&gt; This saying tells me that it depends o...</td>\n",
              "      <td>This saying tells me that it depends on your h...</td>\n",
              "      <td>tell tell tell me that it 's on the way but i ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>322045</th>\n",
              "      <td>She graduates an elementary school .</td>\n",
              "      <td>&lt;start&gt; She is graduating from elementary scho...</td>\n",
              "      <td>She is graduating from elementary school . &lt;end&gt;</td>\n",
              "      <td>a a a in school school</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289399</th>\n",
              "      <td>but the day getting cold . . .</td>\n",
              "      <td>&lt;start&gt; but the days are getting colder . . .</td>\n",
              "      <td>but the days are getting colder . . . &lt;end&gt;</td>\n",
              "      <td>but the weather off cold cold</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193771</th>\n",
              "      <td>And they started to talk in an African language .</td>\n",
              "      <td>&lt;start&gt; They started to talk in an African lan...</td>\n",
              "      <td>They started to talk in an African language . ...</td>\n",
              "      <td>they they to talk to english english english</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156072</th>\n",
              "      <td>For some reason , I came to like active sports .</td>\n",
              "      <td>&lt;start&gt; For some reason , I have come to like ...</td>\n",
              "      <td>For some reason , I have come to like active s...</td>\n",
              "      <td>a a of is is is been to the the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66049</th>\n",
              "      <td>why were I in the middle of the street ?</td>\n",
              "      <td>&lt;start&gt; why was I in the middle of the street ?</td>\n",
              "      <td>why was I in the middle of the street ? &lt;end&gt;</td>\n",
              "      <td>why was the the the earthquake of the earthquake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124938</th>\n",
              "      <td>I want to kill my mom !</td>\n",
              "      <td>&lt;start&gt; I want to kill my mom !</td>\n",
              "      <td>I want to kill my mom !  &lt;end&gt;</td>\n",
              "      <td>i i to do my husband</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>304714</th>\n",
              "      <td>it would be biggest question all the world .</td>\n",
              "      <td>&lt;start&gt; would be biggest question in the world .</td>\n",
              "      <td>would be biggest question in the world . &lt;end&gt;</td>\n",
              "      <td>it it the problem about the world</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107039</th>\n",
              "      <td>The laptop of DELL is breakable .</td>\n",
              "      <td>&lt;start&gt; The laptop of DELL is not durable .</td>\n",
              "      <td>The laptop of DELL is not durable . &lt;end&gt;</td>\n",
              "      <td>the the is is is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>236590</th>\n",
              "      <td>If you have NICO DOUGA ID , you can listen to ...</td>\n",
              "      <td>&lt;start&gt; If you have NICO DOUGA account , you c...</td>\n",
              "      <td>If you have NICO DOUGA account , you can liste...</td>\n",
              "      <td>you you can you you you to to like also to thi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>329576</th>\n",
              "      <td>I was so surprised at myself .</td>\n",
              "      <td>&lt;start&gt; I was so surprised with myself .</td>\n",
              "      <td>I was so surprised with myself . &lt;end&gt;</td>\n",
              "      <td>was was i i i me</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211338</th>\n",
              "      <td>I 'm years old .</td>\n",
              "      <td>&lt;start&gt; I 'm a  year old housewife in Tokyo .</td>\n",
              "      <td>I 'm a  year old housewife in Tokyo . &lt;end&gt;</td>\n",
              "      <td>i i years year old</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85779</th>\n",
              "      <td>But I tired !</td>\n",
              "      <td>&lt;start&gt; But I am tired !</td>\n",
              "      <td>But I am tired ! &lt;end&gt;</td>\n",
              "      <td>i i i tired</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202959</th>\n",
              "      <td>I did not even know how to write a check and h...</td>\n",
              "      <td>&lt;start&gt; I did not even know how to write a che...</td>\n",
              "      <td>I did not even know how to write a check or ho...</td>\n",
              "      <td>did did n't know know how to know the job and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>304605</th>\n",
              "      <td>Tiredness made me so drunk .</td>\n",
              "      <td>&lt;start&gt; My tiredness made me so much more drunk .</td>\n",
              "      <td>My tiredness made me so much more drunk . &lt;end&gt;</td>\n",
              "      <td>i i i me so</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399137</th>\n",
              "      <td>It 's realy amazing</td>\n",
              "      <td>&lt;start&gt; It 's really amazing</td>\n",
              "      <td>It 's really amazing &lt;end&gt;</td>\n",
              "      <td>it it really sad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192563</th>\n",
              "      <td>Their performance is really wonderful .</td>\n",
              "      <td>&lt;start&gt; Their performances are really wonderful .</td>\n",
              "      <td>Their performances are really wonderful . &lt;end&gt;</td>\n",
              "      <td>the the is really nice</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73629</th>\n",
              "      <td>I am arrowed to write message only  letters .</td>\n",
              "      <td>&lt;start&gt; I am allowed to write a message up to ...</td>\n",
              "      <td>I am allowed to write a message up to  letters...</td>\n",
              "      <td>i i trying to write a diary for in write</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129120</th>\n",
              "      <td>Now , there is becoming good .</td>\n",
              "      <td>&lt;start&gt; Now , it is starting to heal .</td>\n",
              "      <td>Now , it is starting to heal . &lt;end&gt;</td>\n",
              "      <td>there there there is a to me me me</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                incorrect  \\\n",
              "14235                   So , I must think fashion sense .   \n",
              "108585  This saying tell me that it depends on your he...   \n",
              "322045               She graduates an elementary school .   \n",
              "289399                     but the day getting cold . . .   \n",
              "193771  And they started to talk in an African language .   \n",
              "156072   For some reason , I came to like active sports .   \n",
              "66049            why were I in the middle of the street ?   \n",
              "124938                            I want to kill my mom !   \n",
              "304714       it would be biggest question all the world .   \n",
              "107039                  The laptop of DELL is breakable .   \n",
              "236590  If you have NICO DOUGA ID , you can listen to ...   \n",
              "329576                     I was so surprised at myself .   \n",
              "211338                                   I 'm years old .   \n",
              "85779                                       But I tired !   \n",
              "202959  I did not even know how to write a check and h...   \n",
              "304605                       Tiredness made me so drunk .   \n",
              "399137                                It 's realy amazing   \n",
              "192563            Their performance is really wonderful .   \n",
              "73629       I am arrowed to write message only  letters .   \n",
              "129120                     Now , there is becoming good .   \n",
              "\n",
              "                                              english_inp  \\\n",
              "14235      <start> So , I must develop my fashion sense .   \n",
              "108585  <start> This saying tells me that it depends o...   \n",
              "322045  <start> She is graduating from elementary scho...   \n",
              "289399      <start> but the days are getting colder . . .   \n",
              "193771  <start> They started to talk in an African lan...   \n",
              "156072  <start> For some reason , I have come to like ...   \n",
              "66049     <start> why was I in the middle of the street ?   \n",
              "124938                   <start> I want to kill my mom !    \n",
              "304714   <start> would be biggest question in the world .   \n",
              "107039        <start> The laptop of DELL is not durable .   \n",
              "236590  <start> If you have NICO DOUGA account , you c...   \n",
              "329576           <start> I was so surprised with myself .   \n",
              "211338      <start> I 'm a  year old housewife in Tokyo .   \n",
              "85779                            <start> But I am tired !   \n",
              "202959  <start> I did not even know how to write a che...   \n",
              "304605  <start> My tiredness made me so much more drunk .   \n",
              "399137                       <start> It 's really amazing   \n",
              "192563  <start> Their performances are really wonderful .   \n",
              "73629   <start> I am allowed to write a message up to ...   \n",
              "129120             <start> Now , it is starting to heal .   \n",
              "\n",
              "                                              english_out  \\\n",
              "14235        So , I must develop my fashion sense . <end>   \n",
              "108585  This saying tells me that it depends on your h...   \n",
              "322045   She is graduating from elementary school . <end>   \n",
              "289399        but the days are getting colder . . . <end>   \n",
              "193771  They started to talk in an African language . ...   \n",
              "156072  For some reason , I have come to like active s...   \n",
              "66049       why was I in the middle of the street ? <end>   \n",
              "124938                     I want to kill my mom !  <end>   \n",
              "304714     would be biggest question in the world . <end>   \n",
              "107039          The laptop of DELL is not durable . <end>   \n",
              "236590  If you have NICO DOUGA account , you can liste...   \n",
              "329576             I was so surprised with myself . <end>   \n",
              "211338        I 'm a  year old housewife in Tokyo . <end>   \n",
              "85779                              But I am tired ! <end>   \n",
              "202959  I did not even know how to write a check or ho...   \n",
              "304605    My tiredness made me so much more drunk . <end>   \n",
              "399137                         It 's really amazing <end>   \n",
              "192563    Their performances are really wonderful . <end>   \n",
              "73629   I am allowed to write a message up to  letters...   \n",
              "129120               Now , it is starting to heal . <end>   \n",
              "\n",
              "                                       PREDICTED_SENTENCE  \n",
              "14235                       do do do do think a favorite   \n",
              "108585  tell tell tell me that it 's on the way but i ...  \n",
              "322045                            a a a in school school   \n",
              "289399                     but the weather off cold cold   \n",
              "193771      they they to talk to english english english   \n",
              "156072                   a a of is is is been to the the   \n",
              "66049   why was the the the earthquake of the earthquake   \n",
              "124938                              i i to do my husband   \n",
              "304714                 it it the problem about the world   \n",
              "107039                                  the the is is is   \n",
              "236590  you you can you you you to to like also to thi...  \n",
              "329576                                  was was i i i me   \n",
              "211338                                i i years year old   \n",
              "85779                                        i i i tired   \n",
              "202959  did did n't know know how to know the job and ...  \n",
              "304605                                       i i i me so   \n",
              "399137                                  it it really sad   \n",
              "192563                            the the is really nice   \n",
              "73629           i i trying to write a diary for in write   \n",
              "129120                there there there is a to me me me   "
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_train['PREDICTED_SENTENCE']= result\n",
        "sample_train.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Gk2shbPIceM"
      },
      "source": [
        "## BLEU Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01z2NYvpJFCO"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBE1SSvSJdxb",
        "outputId": "7a79ab92-5bb7-4084-a2cd-d3c0c9d29dad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "And today , it was last lesson by my english teacher .\n",
            "today today was the last lesson with my english english was was \n",
            "BLEU score: 1.2676114249651737e-231\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\kiran\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "C:\\Users\\kiran\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "C:\\Users\\kiran\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ],
      "source": [
        "inp = sample_train.incorrect.values[20]\n",
        "translate = sample_train.PREDICTED_SENTENCE.values[20]\n",
        "print(inp)\n",
        "print(translate)\n",
        "print('BLEU score: {}'.format(nltk.translate.bleu_score.sentence_bleu(inp, translate)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-w_sHG_HIbNt",
        "outputId": "b32856f2-9095-49d1-fa3c-fe9c81be1a88"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 521.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the average BLEU score of these sentences. are : 1.3058902476517232e-231\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import nltk.translate.bleu_score as bleu\n",
        "\n",
        "score = 0\n",
        "for i in tqdm(range(1000)):\n",
        "    inp = sample_train.incorrect.values[i]\n",
        "\n",
        "    # print(inp)\n",
        "    translate = sample_train.PREDICTED_SENTENCE.values[i]\n",
        "    # print(translate)\n",
        "    bleu = nltk.translate.bleu_score.sentence_bleu(inp, translate)\n",
        "    score = score + bleu\n",
        "average_bleu = score / 1000\n",
        "print('the average BLEU score of these sentences. are :' , average_bleu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwPLy29_IbJ7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "immediate-opinion"
      },
      "source": [
        "# ATTENTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcdUmYYRBbWt"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KqLB3tQ1dxAe",
        "outputId": "e6fbfc59-77b3-4dc1-b05c-671d073839da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.7.0'"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#all imports\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#import tensorflow_hub as hub\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split \n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk import pos_tag \n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Conv2D, Flatten , Input , Conv1D , Concatenate , MaxPooling1D , Dropout , Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import LSTM\n",
        "import datetime\n",
        "\n",
        "from keras.layers import Concatenate\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.layers import Embedding\n",
        "from sklearn.metrics import  f1_score , roc_auc_score\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import nltk.translate.bleu_score as bleu\n",
        "\n",
        "\n",
        "tf.__version__\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU4KIsGxLOfK"
      },
      "source": [
        "### <font color='blue'>**Implement custom encoder decoder and attention layers**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMm3ADQDLOfK"
      },
      "source": [
        "<font color='blue'>**Encoder**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lx_5NA24KzRp"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self,inp_vocab_size,embedding_size,lstm_size,input_length):\n",
        "        super().__init__()\n",
        "        self.inp_vocab_size = inp_vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.input_length = input_length\n",
        "        self.lstm_size= lstm_size\n",
        "        self.lstm_output = 0\n",
        "        self.lstm_state_h=0\n",
        "        self.lstm_state_c=0\n",
        "        \n",
        "        self.embedding = Embedding(input_dim=self.inp_vocab_size, output_dim=self.embedding_size, input_length=self.input_length,\n",
        "                           mask_zero=True, name=\"embedding_layer_encoder\")\n",
        "        self.lstm = LSTM(self.lstm_size, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
        "\n",
        "\n",
        "    def call(self,input_sequence,states):\n",
        "        input_embedd = self.embedding(input_sequence)\n",
        "        self.lstm_output, self.lstm_state_h,self.lstm_state_c = self.lstm(input_embedd, states)\n",
        "        return self.lstm_output, self.lstm_state_h,self.lstm_state_c\n",
        "    \n",
        "    def initialize_states(self,batch_size):\n",
        "      self.lstm_state_h = tf.zeros([batch_size , self.lstm_size])\n",
        "      self.lstm_state_c = tf.zeros([batch_size , self.lstm_size])\n",
        "      return self.lstm_state_h,self.lstm_state_c\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXn278lhLYRM"
      },
      "source": [
        "<font color='blue'>**Attention**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab5SNdPZLlur"
      },
      "outputs": [],
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self,scoring_function, att_units):\n",
        "    super(Attention, self).__init__()\n",
        "    self.scoring_function = scoring_function\n",
        "    self.att_units = att_units\n",
        "    self.W1 = tf.keras.layers.Dense(att_units)\n",
        "    self.W2 = tf.keras.layers.Dense(att_units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  \n",
        "  def call(self,decoder_hidden_state,encoder_output):\n",
        "    \n",
        "    if self.scoring_function == 'dot':\n",
        "        decoder_hidden_state_reshaped = tf.reshape(decoder_hidden_state , (decoder_hidden_state.shape[0],decoder_hidden_state.shape[1],1))\n",
        "\n",
        "        #I WAS USING tf.keras.layers.Dot FOR DOT PRODUCT , BUT IT GAVE INCOMPATIBILITY IN SHAPES , SO NOW I VE USED tf.keras.layers.dot\n",
        "        score =  tf.keras.layers.dot([ encoder_output , decoder_hidden_state_reshaped] , [2,1]) \n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        context_vector = attention_weights * encoder_output  #\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "        pass\n",
        "    elif self.scoring_function == 'general':\n",
        "        decoder_hidden_state_reshaped = tf.reshape(decoder_hidden_state , (decoder_hidden_state.shape[0],decoder_hidden_state.shape[1],1))\n",
        "        W = tf.random.uniform(shape=[encoder_output.shape[0] , self.att_units , self.att_units])\n",
        "        score =  tf.keras.layers.dot([ encoder_output , W] , [2,1]) \n",
        "        score =  tf.keras.layers.dot([ score , decoder_hidden_state_reshaped] , [2,1]) \n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        context_vector = attention_weights * encoder_output  #\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "        pass\n",
        "    elif self.scoring_function == 'concat':\n",
        "\n",
        "        decoder_hidden_state_reshaped = tf.expand_dims(decoder_hidden_state, 1)\n",
        "        score =  self.V(tf.nn.tanh(self.W1(decoder_hidden_state_reshaped) + self.W2(encoder_output)) )\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * encoder_output  #\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "        pass\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic-FNEbfL2DN"
      },
      "source": [
        "<font color='blue'>**OneStepDecoder**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kc8m7lmOL097"
      },
      "outputs": [],
      "source": [
        "class One_Step_Decoder(tf.keras.Model):\n",
        "    def __init__(self,tar_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
        "        super(One_Step_Decoder, self).__init__()\n",
        "\n",
        "        # Initialize decoder embedding layer, LSTM \n",
        "        self.tar_vocab_size = tar_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.input_length = input_length\n",
        "        self.dec_units = dec_units\n",
        "        self.score_fun = score_fun\n",
        "        self.att_units = att_units\n",
        "\n",
        "        self.attention=Attention(score_fun,att_units)\n",
        "        self.embedding = tf.keras.layers.Embedding(tar_vocab_size, embedding_dim)\n",
        "        self.lstm = LSTM(self.dec_units , return_state=True, return_sequences=True, name=\"Decoder_LSTM\")\n",
        "        self.dense = tf.keras.layers.Dense(self.tar_vocab_size)\n",
        "\n",
        "    def call(self,input_to_decoder, encoder_output, state_h,state_c):\n",
        "        output = self.embedding(input_to_decoder) # (32, 1, 12)\n",
        "        context_vector,attention_weights=self.attention(state_h,encoder_output)\n",
        "        concat = tf.concat([tf.expand_dims(context_vector, 1), output], axis=-1)\n",
        "        lstm_output, state_h, state_c = self.lstm(concat)\n",
        "        \n",
        "        output = self.dense(lstm_output)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        return output,state_h,state_c,attention_weights,context_vector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FHrurjUMGAi"
      },
      "source": [
        "<font color='blue'>**Decoder**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwZ2t4tncnUr"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self,out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units):\n",
        "        super(Decoder , self).__init__()\n",
        "        self.out_vocab_size = out_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.input_length = input_length\n",
        "        self.dec_units = dec_units\n",
        "        self.score_fun = score_fun\n",
        "        self.att_units = att_units\n",
        "\n",
        "        self.onestep_decoder=One_Step_Decoder(out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units)\n",
        "\n",
        "    def call(self, input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state ):\n",
        "\n",
        "        all_outputs = tf.TensorArray(tf.float32, size=tf.shape(input_to_decoder)[1])\n",
        "        for timestep in range(tf.shape(input_to_decoder)[1]):\n",
        "            output,state_h,state_c,attention_weights,context_vector = self.onestep_decoder(input_to_decoder[: , timestep : timestep + 1] , \\\n",
        "                                                                                           encoder_output , decoder_hidden_state , decoder_cell_state)\n",
        "\n",
        "\n",
        "            all_outputs = all_outputs.write(timestep , output)\n",
        "        all_outputs = tf.transpose(all_outputs.stack() , [1,0,2])\n",
        "        return all_outputs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3KY8oCFYD51",
        "outputId": "fdc41181-2442-4ec7-939a-88f0aa7d4515"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 10, 13)\n"
          ]
        }
      ],
      "source": [
        "out_vocab_size=13 \n",
        "embedding_dim=12 \n",
        "input_length=10\n",
        "dec_units=16 \n",
        "att_units=16\n",
        "batch_size=32\n",
        "\n",
        "target_sentences=tf.random.uniform(shape=(batch_size,input_length),maxval=10,minval=0,dtype=tf.int32)\n",
        "encoder_output=tf.random.uniform(shape=[batch_size,input_length,dec_units])\n",
        "state_h=tf.random.uniform(shape=[batch_size,dec_units])\n",
        "state_c=tf.random.uniform(shape=[batch_size,dec_units])\n",
        "score_fun = 'concat'\n",
        "decoder=Decoder(out_vocab_size, embedding_dim, input_length, dec_units ,score_fun ,att_units)\n",
        "output=decoder(target_sentences,encoder_output, state_h, state_c)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC1T1EOoMTqC"
      },
      "source": [
        "<font color='blue'>**Encoder Decoder model**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnPvt-oBLNJT"
      },
      "outputs": [],
      "source": [
        "class Dataset:\n",
        "    def __init__(self, data, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, max_len):\n",
        "        self.encoder_inps = data['incorrect'].values\n",
        "        self.decoder_inps = data['english_inp'].values\n",
        "        self.decoder_outs = data['english_out'].values\n",
        "        self.tknizer_CORRECT_SENTENCE = tknizer_CORRECT_SENTENCE\n",
        "        self.tknizer_ERRONEOUS_SENTENCE = tknizer_ERRONEOUS_SENTENCE\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        self.encoder_seq = self.tknizer_ERRONEOUS_SENTENCE.texts_to_sequences([self.encoder_inps[i]]) # need to pass list of values\n",
        "        self.decoder_inp_seq = self.tknizer_CORRECT_SENTENCE.texts_to_sequences([self.decoder_inps[i]])\n",
        "        self.decoder_out_seq = self.tknizer_CORRECT_SENTENCE.texts_to_sequences([self.decoder_outs[i]])\n",
        "\n",
        "        self.encoder_seq = pad_sequences(self.encoder_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
        "        self.decoder_inp_seq = pad_sequences(self.decoder_inp_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
        "        self.decoder_out_seq = pad_sequences(self.decoder_out_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
        "        return self.encoder_seq, self.decoder_inp_seq, self.decoder_out_seq\n",
        "\n",
        "    def __len__(self): # your model.fit_gen requires this function\n",
        "        return len(self.encoder_inps)\n",
        "\n",
        "\n",
        "class Dataloder(tf.keras.utils.Sequence):\n",
        "    \n",
        "    def __init__(self, dataset, batch_size=1):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.indexes = np.arange(len(self.dataset.encoder_inps))\n",
        "\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        start = i * self.batch_size\n",
        "        stop = (i + 1) * self.batch_size\n",
        "        data = []\n",
        "        for j in range(start, stop):\n",
        "            data.append(self.dataset[j])\n",
        "\n",
        "        batch = [np.squeeze(np.stack(samples, axis=1), axis=0) for samples in zip(*data)]\n",
        "        \n",
        "        # we are creating data like ([italian, english_inp], english_out) these are already converted into seq\n",
        "        \n",
        "        return [batch[0],batch[1]],batch[2]\n",
        "\n",
        "    def __len__(self):  # your model.fit_gen requires this function\n",
        "        return len(self.indexes) // self.batch_size\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = np.random.permutation(self.indexes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tAlrQSNLNJV",
        "outputId": "faba10fe-d331-4366-a127-da33bfe5d6dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(512, 20) (512, 20) (512, 20)\n"
          ]
        }
      ],
      "source": [
        "train_dataset = Dataset(train, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, 16)\n",
        "test_dataset  = Dataset(validation, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, 16)\n",
        "\n",
        "train_dataset = Dataset(train, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, 20)\n",
        "test_dataset  = Dataset(validation, tknizer_ERRONEOUS_SENTENCE, tknizer_CORRECT_SENTENCE, 20)\n",
        "\n",
        "train_dataloader = Dataloder(train_dataset, batch_size=512)\n",
        "test_dataloader = Dataloder(test_dataset, batch_size=512)\n",
        "\n",
        "print(train_dataloader[0][0][0].shape, train_dataloader[0][0][1].shape, train_dataloader[0][1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfqBIe20MT3D"
      },
      "outputs": [],
      "source": [
        "class encoder_decoder(tf.keras.Model):\n",
        "    def __init__(self,score_fun , encoder_inputs_length,decoder_inputs_length, output_vocab_size):\n",
        "        super().__init__()\n",
        "        #encoder decoder\n",
        "\n",
        "        self.score_fun = score_fun\n",
        "\n",
        "        self.encoder=Encoder(inp_vocab_size = vocab_size_ERRONEOUS_SENTENCE+1,embedding_size = 50,lstm_size = 64,input_length = encoder_inputs_length)\n",
        "        self.decoder=Decoder(out_vocab_size = vocab_size_CORRECT_SENTENCE+1, embedding_dim = 100, input_length = decoder_inputs_length, dec_units =  64 \\\n",
        "                             ,score_fun = self.score_fun ,att_units = 64)\n",
        "\n",
        "    def call(self,data):\n",
        "\n",
        "        input,output = data[0], data[1]\n",
        "        initial_state= self.encoder.initialize_states(batch_size)\n",
        "        encoder_output, encoder_h, encoder_c = self.encoder(input , initial_state)\n",
        "\n",
        "        decoder_output= self.decoder(output,encoder_output, encoder_h, encoder_c)\n",
        "\n",
        "        return decoder_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TV5Vr3yiNEpC",
        "outputId": "90a447b0-2057-4a71-fa98-1e9d46f0c6d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\kiran\\AppData\\Local\\Temp\\ipykernel_25572\\2738313064.py:12: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model_2.fit_generator(train_dataloader, steps_per_epoch=train_steps, epochs=15, validation_data=train_dataloader, validation_steps=valid_steps , callbacks = checkpoint)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "320/320 [==============================] - 4301s 13s/step - loss: 4.2143 - val_loss: 3.8058\n",
            "Epoch 2/15\n",
            " 64/320 [=====>........................] - ETA: 54:48 - loss: 3.9147"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Input \u001b[1;32mIn [102]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m log_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs1\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(log_dir\u001b[38;5;241m=\u001b[39mlog_dir,histogram_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, write_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,write_grads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 12\u001b[0m \u001b[43mmodel_2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_steps\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# model_1.fit_generator(train_dataloader,  epochs=4, validation_data=train_dataloader)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m model_2\u001b[38;5;241m.\u001b[39msummary()\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:2016\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;124;03m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[0;32m   2006\u001b[0m \n\u001b[0;32m   2007\u001b[0m \u001b[38;5;124;03mDEPRECATED:\u001b[39;00m\n\u001b[0;32m   2008\u001b[0m \u001b[38;5;124;03m  `Model.fit` now supports generators, so there is no longer any need to use\u001b[39;00m\n\u001b[0;32m   2009\u001b[0m \u001b[38;5;124;03m  this endpoint.\u001b[39;00m\n\u001b[0;32m   2010\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2011\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   2012\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`Model.fit_generator` is deprecated and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2013\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwill be removed in a future version. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2014\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   2015\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m-> 2016\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2018\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2028\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2030\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1216\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1209\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1210\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1211\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1212\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1213\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1214\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1215\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1216\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1217\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1218\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:910\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    907\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 910\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    912\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    913\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:942\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    939\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    940\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    941\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 942\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3130\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3127\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   3128\u001b[0m   (graph_function,\n\u001b[0;32m   3129\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1959\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1955\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1957\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1958\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1959\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1960\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1961\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1962\u001b[0m     args,\n\u001b[0;32m   1963\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1964\u001b[0m     executing_eagerly)\n\u001b[0;32m   1965\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:598\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 598\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    604\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    605\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    606\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    607\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    610\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    611\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
            "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:58\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 58\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     61\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "batch_size=512\n",
        "score_fun  = 'general'\n",
        "att_units = 64\n",
        "model_2  = encoder_decoder(score_fun = score_fun , encoder_inputs_length=20,decoder_inputs_length=10,output_vocab_size=vocab_size_CORRECT_SENTENCE)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "model_2.compile(optimizer=optimizer,loss='sparse_categorical_crossentropy')\n",
        "train_steps=train.shape[0]//1024\n",
        "valid_steps=validation.shape[0]//1024\n",
        "#TRANING THE MODEL FOR 20 EPOCHS CAUSE , MORE TRAINING GIVES MORE RESULTS\n",
        "log_dir=\"logs1\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "checkpoint = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True,write_grads=True)\n",
        "model_2.fit_generator(train_dataloader, steps_per_epoch=train_steps, epochs=15, validation_data=train_dataloader, validation_steps=valid_steps , callbacks = checkpoint)\n",
        "# model_1.fit_generator(train_dataloader,  epochs=4, validation_data=train_dataloader)\n",
        "model_2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwuT0ilOWyep"
      },
      "outputs": [],
      "source": [
        "os.mkdir('saved_model_attention')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nBeay4BWxZm"
      },
      "outputs": [],
      "source": [
        "model_2.save_weights('saved_model_attention/attention.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fkn1yHlWHjJQ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_mc_2MyWxSx",
        "outputId": "52d00bd1-fd00-4fce-ba11-01ae4167888d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"encoder_decoder_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "encoder_1 (Encoder)          multiple                  4347640   \n",
            "_________________________________________________________________\n",
            "decoder_2 (Decoder)          multiple                  5753269   \n",
            "=================================================================\n",
            "Total params: 10,100,909\n",
            "Trainable params: 10,100,909\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_2.load_weights(\"/content/drive/MyDrive/saved_model_attention/attention.h5\")\n",
        "model_2.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DpC9zlzMcXp"
      },
      "source": [
        "## <font color='blue'>**Inference**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1IhdBrgQYJr"
      },
      "source": [
        "<font color='blue'>**CORRECTING THE INCORRECT SENTENCES**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MP3kLZoPMvSu",
        "outputId": "3670fdec-f808-4b2b-8025-4cb4859ae1c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "These wire works was made by my friend of next seat in university .\n",
            "i will meet their first i will meet their first i will meet their first i will meet their first\n",
            "BLEU score: 1.0377133938315695e-231\n"
          ]
        }
      ],
      "source": [
        "def predict(input_sentence):\n",
        "    pred = []\n",
        "    input_sequence = tknizer_CORRECT_SENTENCE.texts_to_sequences([input_sentence])\n",
        "    result = ' '\n",
        "    encoder_seq = pad_sequences(input_sequence, maxlen=20, dtype='int32', padding='post')  \n",
        "    initial_state = model_2.layers[0].initialize_states(1)\n",
        "    encoder_output, encoder_h, encoder_c = model_2.layers[0](tf.constant(encoder_seq), initial_state)\n",
        "    start_index = tf.constant([[tknizer_CORRECT_SENTENCE.word_index['<start>']]])\n",
        "    states = [encoder_h, encoder_c]\n",
        "\n",
        "    for i in range(20): \n",
        "        # print(start_index)\n",
        "        predicted_out,state_h,state_c,attention_weights,context_vector = model_2.layers[1].onestep_decoder(start_index, encoder_output , encoder_h, encoder_c )\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        start_index = np.reshape(np.argmax(predicted_out), (1, 1))\n",
        "        pred.append(tknizer_CORRECT_SENTENCE.index_word[start_index[0][0]])\n",
        "        if(pred[-1]=='<end>'):\n",
        "            break\n",
        "        translated_sentence = ' '.join(pred)\n",
        "    return translated_sentence\n",
        "\n",
        "inp = validation.values[2000][0]\n",
        "print(inp)\n",
        "translate = predict(inp)\n",
        "print(translate)\n",
        "print('BLEU score: {}'.format(bleu.sentence_bleu(inp, translate)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OH25UaQH1DZ",
        "outputId": "b5fa7d6b-7669-4319-9cc1-1f242202638c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "At  .  a . m .\n",
            "i want\n",
            "BLEU score: 1.5319719891192393e-231\n"
          ]
        }
      ],
      "source": [
        "inp = validation.values[2][0]\n",
        "print(inp)\n",
        "translate = predict(inp)\n",
        "print(translate)\n",
        "print('BLEU score: {}'.format(bleu.sentence_bleu(inp, translate)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prdmzWvhH1Db",
        "outputId": "b752e656-dcd8-4c13-d563-f1d45fb56911"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How can I do for she ? ? ? ?\n",
            "i will have as i will have as i will have as i will have as i will have as\n",
            "BLEU score: 9.72161026064145e-232\n"
          ]
        }
      ],
      "source": [
        "inp = validation.values[20][0]\n",
        "print(inp)\n",
        "translate = predict(inp)\n",
        "print(translate)\n",
        "print('BLEU score: {}'.format(bleu.sentence_bleu(inp, translate)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItU3DRZ_H1Dc",
        "outputId": "ee826c0c-b89a-4d18-f313-8cab06fb9d40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "These wire works was made by my friend of next seat in university .\n",
            "i will meet their first i will meet their first i will meet their first i will meet their first\n",
            "BLEU score: 1.0377133938315695e-231\n"
          ]
        }
      ],
      "source": [
        "inp = validation.values[2000][0]\n",
        "print(inp)\n",
        "translate = predict(inp)\n",
        "print(translate)\n",
        "print('BLEU score: {}'.format(bleu.sentence_bleu(inp, translate)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6KDp62yH1Dd"
      },
      "outputs": [],
      "source": [
        "#USING THIS METHOD TO GET THE BLEU SCORE , THE REFERENCE NOTEBOOK METHOD SHOWS ME AN ERROR FOR 1000 FILES , BUT IT WORKS FINE FOR SINGLE FILE\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnnOeacuH1De",
        "outputId": "6990980e-94e3-4188-fd6c-17841a8e9cc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "He s just started at the Ministry and this is\n",
            "i have the\n",
            "BLEU score: 0.8801117367933934\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ],
      "source": [
        "inp = validation.values[2][0]\n",
        "print(inp)\n",
        "translate = predict(inp)\n",
        "print(translate)\n",
        "print('BLEU score: {}'.format(nltk.translate.bleu_score.sentence_bleu(inp, translate)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiFzVJGhXUr6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9F0i-eyyXUmV",
        "outputId": "def2f7d0-1454-431c-88fe-31ff16f3e063"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:22<00:00, 12.12it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['i want to make a as a as a as a as a as a as a as a as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will have their first i will have their first i will have their first i will have their first',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will have their first i will have their first i will have their first i will have their first',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i will meet',\n",
              " 'i will be as i will meet their first i will be as i will be as i will be',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will be as a as a as a as a as a as a as a as a as',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet a as i will meet a as i will meet a as i will meet a as',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will be as their first i will be as their first i will be as their first i will',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i will meet',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet',\n",
              " 'i will meet',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i will meet',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will be as the first i will be as the first i will be as the first i will',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i will meet',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i will meet',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i want',\n",
              " 'i will have a as i will have a as i will have a as i will have a as',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will be as their first i will be as their first i will be as their first i will',\n",
              " 'i want',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i will be as the first i will be as the first i will be as the first i will',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want to make their as i want to make their as i want to make their as i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will have as i will have as i will have as i will have as i will have a',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i will be as as as as as as i will be as as as as as as as as',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i will meet',\n",
              " 'i will meet',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i will meet',\n",
              " 'i will be as their first i will be as their first i will be as their first i will',\n",
              " 'i want',\n",
              " 'i will be as as as as as as as as as as as as as as as as as',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i will be as as as as as as as as as as as as as as as as as',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to their first i want to their first i want to their first i want to their first',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i will meet',\n",
              " 'i want to make a as a as a as a as a as a as a as a as',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet a as i will meet a as i will meet a as i will meet a as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet a as i will meet a as i will meet a as i will meet a as',\n",
              " 'i will meet',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i will have as their first i will have as their first i will have as their first i will',\n",
              " 'i want',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i will meet',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will have their as i will have their as i will have their as i will have their as',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i will have as their as their as their as their as their as their as their as their as',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i want',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i want',\n",
              " 'i will be as the first i will meet their first i will meet their first i will meet their',\n",
              " 'i will meet',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i will meet',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i will meet',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i will meet',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will have their first i will have their first i will have their first i will have their first',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will have as their first i will have as their first i will have as their first i will',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet a as i will meet a as i will meet a as i will meet a as',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i will meet',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will have a as i will have a as i will have a as i will have a as',\n",
              " 'i want',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i will be as their first i will be as their first i will be as their first i will',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i want to make a as a as a as a as a as a as a as a as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make a as a as a as a as a as a as a as a as',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want to make their as i want to make their as i want to make their as i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i will meet',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i will have a as i will have a as i will have a as i will have a as',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i want',\n",
              " 'i will be as a as a as a as a as a as a as a as a as',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i will meet',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i want',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i will meet',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet a as i will meet a as i will meet a as i will meet a as',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make their first i want to make their first i want to make their first i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will be as their first i will be as their first i will be as their first i will',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will have as their as their as their as their as their as their as their as their as',\n",
              " 'i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i want',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i will be as as as as as as as as as as as as as as as as as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i will have as i will have as i will be as i will be as i will be as',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i will meet a as a as a as a as a as a as a as a as a',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will have a as a as a as a as a as a as a as a as a',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want to make a as a as a as a as a as a as a as a as',\n",
              " 'i want',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i will meet',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i want to make their as i want to make their as i want to make their as i want',\n",
              " 'i will meet',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i will have a as i will have a as i will have a as i will have a as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i will have as i will have as i will have as i will have as i will have as',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i will meet',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i will be as i will be as i will be as i will be as i will be as',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will have a as i will have a as i will have a as i will have a as',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will have as as as as as as as as as as as as as as as as as',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will have a as i will have a as i will have a as i will have a as',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i will have as their as their as their as their as their as their as their as their as',\n",
              " 'i want',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will have a as i will have a as i will have a as i will have a as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i will meet',\n",
              " 'i will meet',\n",
              " 'i want to make a as i want to make a as i want to make a as i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their as i will meet their as i will meet their as i will meet their as',\n",
              " 'i want',\n",
              " 'i want',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i will meet',\n",
              " 'i will meet',\n",
              " 'i will meet their first i will meet their first i will meet their first i will meet their first',\n",
              " 'i want']"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_train = train.sample(1000)\n",
        "result = []\n",
        "i=0\n",
        "for i, j , k in tqdm(sample_train.values):\n",
        "    # print(i , j , k)\n",
        "    pred = predict(i)\n",
        "    result.append(pred)\n",
        "\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "id": "vpMHogB_XyUc",
        "outputId": "1f9c104c-e00e-4790-b450-eea13359c4f4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>incorrect</th>\n",
              "      <th>english_inp</th>\n",
              "      <th>english_out</th>\n",
              "      <th>PREDICTED_SENTENCE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>177186</th>\n",
              "      <td>One of my girlfriends terribly frightened</td>\n",
              "      <td>&lt;start&gt; One of my girlfriends was terribly fri...</td>\n",
              "      <td>One of my girlfriends was terribly frightened ...</td>\n",
              "      <td>i want to make a as a as a as a as a as a as a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181690</th>\n",
              "      <td>She becomes two grades from this April .</td>\n",
              "      <td>&lt;start&gt; She join the second grade from this Ap...</td>\n",
              "      <td>She join the second grade from this April . &lt;end&gt;</td>\n",
              "      <td>i want</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275101</th>\n",
              "      <td>Stickam is a web service where we can broadcas...</td>\n",
              "      <td>&lt;start&gt; Stickam is a web service where we can ...</td>\n",
              "      <td>Stickam is a web service where we can broadcas...</td>\n",
              "      <td>i want</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251982</th>\n",
              "      <td>I feeled very sad .</td>\n",
              "      <td>&lt;start&gt; I felt very sad .</td>\n",
              "      <td>I felt very sad . &lt;end&gt;</td>\n",
              "      <td>i want</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27655</th>\n",
              "      <td>The night of the day , I do n't slept because ...</td>\n",
              "      <td>&lt;start&gt; At night , I was n't able to sleep bec...</td>\n",
              "      <td>At night , I was n't able to sleep because I t...</td>\n",
              "      <td>i will have their first i will have their firs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>383240</th>\n",
              "      <td>From this generation , they could speak Englis...</td>\n",
              "      <td>&lt;start&gt; From this generation , they could spea...</td>\n",
              "      <td>From this generation , they could speak Englis...</td>\n",
              "      <td>i want</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>350801</th>\n",
              "      <td>and I 'll write congratulations letter to her .</td>\n",
              "      <td>&lt;start&gt; and I 'll write a congratulations lett...</td>\n",
              "      <td>and I 'll write a congratulations letter for h...</td>\n",
              "      <td>i want</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>221737</th>\n",
              "      <td>some were too hot .</td>\n",
              "      <td>&lt;start&gt; Some was too hot .</td>\n",
              "      <td>Some was too hot . &lt;end&gt;</td>\n",
              "      <td>i will have their first i will have their firs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149709</th>\n",
              "      <td>OMG ! ! ! I almost missed the th . Chinese Bri...</td>\n",
              "      <td>&lt;start&gt; OMG ! ! ! I almost missed the th  Chin...</td>\n",
              "      <td>OMG ! ! ! I almost missed the th  Chinese Brid...</td>\n",
              "      <td>i want</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108845</th>\n",
              "      <td>Today , e  mail become estalished among many ...</td>\n",
              "      <td>&lt;start&gt;  Today , e  mail becomes estalished am...</td>\n",
              "      <td>Today , e  mail becomes estalished among many...</td>\n",
              "      <td>i want</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125182</th>\n",
              "      <td>It was a long time ago , then I 've started to...</td>\n",
              "      <td>&lt;start&gt; It was a long time ago that I started ...</td>\n",
              "      <td>It was a long time ago that I started to learn...</td>\n",
              "      <td>i want</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>237084</th>\n",
              "      <td>If you select a rhythmic song , It 's all right !</td>\n",
              "      <td>&lt;start&gt; So if you select a rhythmic song , it ...</td>\n",
              "      <td>So if you select a rhythmic song , it 's ok ! ...</td>\n",
              "      <td>i will meet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165525</th>\n",
              "      <td>house because of the mortgage .</td>\n",
              "      <td>&lt;start&gt; homes because of their inability to pa...</td>\n",
              "      <td>homes because of their inability to pay their ...</td>\n",
              "      <td>i will meet their first i will meet their firs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199580</th>\n",
              "      <td>I hope our ocarina sound made patients happy a...</td>\n",
              "      <td>&lt;start&gt; I hope our ocarina sound made patients...</td>\n",
              "      <td>I hope our ocarina sound made patients happy a...</td>\n",
              "      <td>i will meet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362451</th>\n",
              "      <td>As my pet walk around me , his foot put down o...</td>\n",
              "      <td>&lt;start&gt; As my pet walked around me , he put hi...</td>\n",
              "      <td>As my pet walked around me , he put his foot d...</td>\n",
              "      <td>i want</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180184</th>\n",
              "      <td>today I would like speaking of the cities .</td>\n",
              "      <td>&lt;start&gt; Today I would like to write about citi...</td>\n",
              "      <td>Today I would like to write about cities . &lt;end&gt;</td>\n",
              "      <td>i will meet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140302</th>\n",
              "      <td>I watched TV show yesterday .</td>\n",
              "      <td>&lt;start&gt; I watched a TV show yesterday .</td>\n",
              "      <td>I watched a TV show yesterday . &lt;end&gt;</td>\n",
              "      <td>i want to make a as i want to make a as i want...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104799</th>\n",
              "      <td>I want to play my guitar , it does relieves th...</td>\n",
              "      <td>&lt;start&gt; I want to play my guitar , it relieves...</td>\n",
              "      <td>I want to play my guitar , it relieves the bla...</td>\n",
              "      <td>i will meet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414603</th>\n",
              "      <td>Characteristic of the comics is it has lots of...</td>\n",
              "      <td>&lt;start&gt; The characteristic of the comics is th...</td>\n",
              "      <td>The characteristic of the comics is that it ha...</td>\n",
              "      <td>i will meet their as i will meet their as i wi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56587</th>\n",
              "      <td>Finally , bike is obviously cheap .</td>\n",
              "      <td>&lt;start&gt; Finally , a bicycle is obviously cheap...</td>\n",
              "      <td>Finally , a bicycle is obviously cheaper . &lt;end&gt;</td>\n",
              "      <td>i want</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                incorrect  \\\n",
              "177186          One of my girlfriends terribly frightened   \n",
              "181690           She becomes two grades from this April .   \n",
              "275101  Stickam is a web service where we can broadcas...   \n",
              "251982                                I feeled very sad .   \n",
              "27655   The night of the day , I do n't slept because ...   \n",
              "383240  From this generation , they could speak Englis...   \n",
              "350801    and I 'll write congratulations letter to her .   \n",
              "221737                                some were too hot .   \n",
              "149709  OMG ! ! ! I almost missed the th . Chinese Bri...   \n",
              "108845   Today , e  mail become estalished among many ...   \n",
              "125182  It was a long time ago , then I 've started to...   \n",
              "237084  If you select a rhythmic song , It 's all right !   \n",
              "165525                    house because of the mortgage .   \n",
              "199580  I hope our ocarina sound made patients happy a...   \n",
              "362451  As my pet walk around me , his foot put down o...   \n",
              "180184        today I would like speaking of the cities .   \n",
              "140302                      I watched TV show yesterday .   \n",
              "104799  I want to play my guitar , it does relieves th...   \n",
              "414603  Characteristic of the comics is it has lots of...   \n",
              "56587                 Finally , bike is obviously cheap .   \n",
              "\n",
              "                                              english_inp  \\\n",
              "177186  <start> One of my girlfriends was terribly fri...   \n",
              "181690  <start> She join the second grade from this Ap...   \n",
              "275101  <start> Stickam is a web service where we can ...   \n",
              "251982                          <start> I felt very sad .   \n",
              "27655   <start> At night , I was n't able to sleep bec...   \n",
              "383240  <start> From this generation , they could spea...   \n",
              "350801  <start> and I 'll write a congratulations lett...   \n",
              "221737                         <start> Some was too hot .   \n",
              "149709  <start> OMG ! ! ! I almost missed the th  Chin...   \n",
              "108845  <start>  Today , e  mail becomes estalished am...   \n",
              "125182  <start> It was a long time ago that I started ...   \n",
              "237084  <start> So if you select a rhythmic song , it ...   \n",
              "165525  <start> homes because of their inability to pa...   \n",
              "199580  <start> I hope our ocarina sound made patients...   \n",
              "362451  <start> As my pet walked around me , he put hi...   \n",
              "180184  <start> Today I would like to write about citi...   \n",
              "140302            <start> I watched a TV show yesterday .   \n",
              "104799  <start> I want to play my guitar , it relieves...   \n",
              "414603  <start> The characteristic of the comics is th...   \n",
              "56587   <start> Finally , a bicycle is obviously cheap...   \n",
              "\n",
              "                                              english_out  \\\n",
              "177186  One of my girlfriends was terribly frightened ...   \n",
              "181690  She join the second grade from this April . <end>   \n",
              "275101  Stickam is a web service where we can broadcas...   \n",
              "251982                            I felt very sad . <end>   \n",
              "27655   At night , I was n't able to sleep because I t...   \n",
              "383240  From this generation , they could speak Englis...   \n",
              "350801  and I 'll write a congratulations letter for h...   \n",
              "221737                           Some was too hot . <end>   \n",
              "149709  OMG ! ! ! I almost missed the th  Chinese Brid...   \n",
              "108845   Today , e  mail becomes estalished among many...   \n",
              "125182  It was a long time ago that I started to learn...   \n",
              "237084  So if you select a rhythmic song , it 's ok ! ...   \n",
              "165525  homes because of their inability to pay their ...   \n",
              "199580  I hope our ocarina sound made patients happy a...   \n",
              "362451  As my pet walked around me , he put his foot d...   \n",
              "180184   Today I would like to write about cities . <end>   \n",
              "140302              I watched a TV show yesterday . <end>   \n",
              "104799  I want to play my guitar , it relieves the bla...   \n",
              "414603  The characteristic of the comics is that it ha...   \n",
              "56587    Finally , a bicycle is obviously cheaper . <end>   \n",
              "\n",
              "                                       PREDICTED_SENTENCE  \n",
              "177186  i want to make a as a as a as a as a as a as a...  \n",
              "181690                                             i want  \n",
              "275101                                             i want  \n",
              "251982                                             i want  \n",
              "27655   i will have their first i will have their firs...  \n",
              "383240                                             i want  \n",
              "350801                                             i want  \n",
              "221737  i will have their first i will have their firs...  \n",
              "149709                                             i want  \n",
              "108845                                             i want  \n",
              "125182                                             i want  \n",
              "237084                                        i will meet  \n",
              "165525  i will meet their first i will meet their firs...  \n",
              "199580                                        i will meet  \n",
              "362451                                             i want  \n",
              "180184                                        i will meet  \n",
              "140302  i want to make a as i want to make a as i want...  \n",
              "104799                                        i will meet  \n",
              "414603  i will meet their as i will meet their as i wi...  \n",
              "56587                                              i want  "
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_train['PREDICTED_SENTENCE']= result\n",
        "sample_train.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmxIVOOQPWMu"
      },
      "source": [
        "<font color='blue'>**Calculate BLEU score**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iHiLdROM23l",
        "outputId": "eca5a2c2-4a93-4477-c1ea-a61fdeb2ff3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU score: 0\n"
          ]
        }
      ],
      "source": [
        "#Create an object of your custom model.\n",
        "#Compile and train your model on dot scoring function.\n",
        "# Visualize few sentences randomly in Test data\n",
        "# Predict on 1000 random sentences on test data and calculate the average BLEU score of these sentences.\n",
        "# https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n",
        "\n",
        "#Sample example\n",
        "import nltk.translate.bleu_score as bleu\n",
        "reference = ['i am groot'.split(),] # the original\n",
        "translation = 'it is ship'.split() # trasilated using model\n",
        "print('BLEU score: {}'.format(bleu.sentence_bleu(reference, translation)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5Y6JrIxIao5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "GRAMMAR_ERROR_HANDLING.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}